{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About This is the documentation of The Butterfly Director (TBD) group, for the Imperial College London, Design Engineering Third Year (DE3), Robotics module (March 2020). Team Harvey Upton Tom\u00e1\u0161 K\u0148aze David Prior Hope Pauline Ng Luke Hillery Higor Alves De Freitas For enquiries on this documentation, please contact meng.ng17@imperial.ac.uk For help with our code, please contact either harvey.upton17@imperial.ac.uk or tomas.knaze15@imperial.ac.uk Summary Drawing inspiration from light painting images, this project entails programming the Franka Emika robotic arm to draw several still images of a butterfly with light to produce a stop motion animation. The Franka Emika was used in comparison to the DE NIRO robot because it is more precise in terms of execution, which is more suitable for this project.","title":"Introduction"},{"location":"#about","text":"This is the documentation of The Butterfly Director (TBD) group, for the Imperial College London, Design Engineering Third Year (DE3), Robotics module (March 2020).","title":"About"},{"location":"#team","text":"Harvey Upton Tom\u00e1\u0161 K\u0148aze David Prior Hope Pauline Ng Luke Hillery Higor Alves De Freitas For enquiries on this documentation, please contact meng.ng17@imperial.ac.uk For help with our code, please contact either harvey.upton17@imperial.ac.uk or tomas.knaze15@imperial.ac.uk","title":"Team"},{"location":"#summary","text":"Drawing inspiration from light painting images, this project entails programming the Franka Emika robotic arm to draw several still images of a butterfly with light to produce a stop motion animation. The Franka Emika was used in comparison to the DE NIRO robot because it is more precise in terms of execution, which is more suitable for this project.","title":"Summary"},{"location":"CodeExecutionInSimulation/","text":"Code Execution in Simulation This section explains how to run the python code on a Panda Robot Simulation. Before attempting to light paint using a real robot it is recomended to first get it working in the Gazebo simulator within your virtual machine. This allows you to test your custom animations and verify everything is working without wasting time on a real robot. 1. Downloading Code Once you have completed the previous steps to set up ROS on your computer, with the required modules,you can now use the light painting code in the Gazebo simulator. Up to date files can be found on GitHub at this link . Download pandaLightPaint.py and frames.py onto your linux virtual machine. 2. Launching Gazebo Open a new terminator window and divide into 6 windows and type in the follwing. In terminal 1 type $roscore In terminal 2 type $cd catkin_ws $cd source devel/setup.bash $rosrun gazebo_ros gazebo In terminal 3 type $cd catkin_ws $source devel/setup.bash $roslaunch franka_gazebo panda_arm_hand.launch You should see that the gazebo simulator has launched and a model of the panda robot can be seen pointing upwards in the environment. 3. Launching MoveIt To launch and set up MoveIt for motion planning do the following In terminal 4 type $cd_catkin_ws $source devel/setup.bash $roslaunch panda_moveit_config demo.launch rviz_tutorial:=true One MoveIt has started add motion planning and change the planning scene topic field to /planning_scene In planning request change planning group to panda_arm 4. Light Trails in MoveIt By tracing the position of the end effector in RViz, it allows the path of the attached light-painting module to be recreated in the RViz viewport which helps when testing new animations. This effectively generates a digital model of the light painting that the Panda robot would recreate in real life. To initialize the light tracing simulation in RViz, go to: 1. Motion Planning 2. Planned Path 3. Links 4. Check Panda_Hand[_] 5. Tick the box Show Trail [_] When the simulation is run in RViz you will see a line which shows the trajectory the end effector will move along. 5. Connect MoveIt to Gazebo To create the node which connects moveIt with Gazebo type the following In terminal 5 type $cd catkin_ws $cd src/panda_publisher $python panda_publisher.py 6. launching pandaLightPaint Everything is now set for the simulation so you can launch the pandaLightPaint python code In terminal 6 type $cd catkin_ws cd to the folder you stored the code in $python pandaLightPaint.py The first time you run this you will get some error messages as the robot starts at singularity in the simulation. After about a minute it will find a valid motion plan to get out of singularity. At this point cancel the execution using ctrl c then start the code again. It should now run properly without errors. You should see the robot drawing individual frames in both gazebo and RViz and pausing in between each frame to allow the photo to be processed.","title":"Simulation"},{"location":"CodeExecutionInSimulation/#code-execution-in-simulation","text":"This section explains how to run the python code on a Panda Robot Simulation. Before attempting to light paint using a real robot it is recomended to first get it working in the Gazebo simulator within your virtual machine. This allows you to test your custom animations and verify everything is working without wasting time on a real robot.","title":"Code Execution in Simulation"},{"location":"CodeExecutionInSimulation/#1-downloading-code","text":"Once you have completed the previous steps to set up ROS on your computer, with the required modules,you can now use the light painting code in the Gazebo simulator. Up to date files can be found on GitHub at this link . Download pandaLightPaint.py and frames.py onto your linux virtual machine.","title":"1. Downloading Code"},{"location":"CodeExecutionInSimulation/#2-launching-gazebo","text":"Open a new terminator window and divide into 6 windows and type in the follwing. In terminal 1 type $roscore In terminal 2 type $cd catkin_ws $cd source devel/setup.bash $rosrun gazebo_ros gazebo In terminal 3 type $cd catkin_ws $source devel/setup.bash $roslaunch franka_gazebo panda_arm_hand.launch You should see that the gazebo simulator has launched and a model of the panda robot can be seen pointing upwards in the environment.","title":"2. Launching Gazebo"},{"location":"CodeExecutionInSimulation/#3-launching-moveit","text":"To launch and set up MoveIt for motion planning do the following In terminal 4 type $cd_catkin_ws $source devel/setup.bash $roslaunch panda_moveit_config demo.launch rviz_tutorial:=true One MoveIt has started add motion planning and change the planning scene topic field to /planning_scene In planning request change planning group to panda_arm","title":"3. Launching MoveIt"},{"location":"CodeExecutionInSimulation/#4-light-trails-in-moveit","text":"By tracing the position of the end effector in RViz, it allows the path of the attached light-painting module to be recreated in the RViz viewport which helps when testing new animations. This effectively generates a digital model of the light painting that the Panda robot would recreate in real life. To initialize the light tracing simulation in RViz, go to: 1. Motion Planning 2. Planned Path 3. Links 4. Check Panda_Hand[_] 5. Tick the box Show Trail [_] When the simulation is run in RViz you will see a line which shows the trajectory the end effector will move along.","title":"4. Light Trails in MoveIt"},{"location":"CodeExecutionInSimulation/#5-connect-moveit-to-gazebo","text":"To create the node which connects moveIt with Gazebo type the following In terminal 5 type $cd catkin_ws $cd src/panda_publisher $python panda_publisher.py","title":"5. Connect MoveIt to Gazebo"},{"location":"CodeExecutionInSimulation/#6-launching-pandalightpaint","text":"Everything is now set for the simulation so you can launch the pandaLightPaint python code In terminal 6 type $cd catkin_ws cd to the folder you stored the code in $python pandaLightPaint.py The first time you run this you will get some error messages as the robot starts at singularity in the simulation. After about a minute it will find a valid motion plan to get out of singularity. At this point cancel the execution using ctrl c then start the code again. It should now run properly without errors. You should see the robot drawing individual frames in both gazebo and RViz and pausing in between each frame to allow the photo to be processed.","title":"6. launching pandaLightPaint"},{"location":"Image/","text":"Image Capture To ensure the light painting will be visible, the surrounding lights should be turned off to increase contrast, while using long exposure photography to capture each frame. Set the camera ISO to 100 to reduce the graininess of images. To reduce light entering the camera and increasing contrast, set the aperture to f/22. Ensure the entire image is in focus by using ensuring the depth of field is at its maximum depending on your camera. This can be calculated here . Mount the camera on a tripod with an external camera remote to avoid camera shake. Set the camera on \u201ctime\u201d mode, so that one press of the shutter would open the aperture and another would close the aperture. As the duration of each frame varies, this would provide a simple error-prood method. Before taking photos, focus the camera on the origin of the end-effector using autofocus and then put on manual focus for the rest of the images. With the code provided, Franka Emika would signal when to press the shutter by wiggling before and after each frame begins.","title":"Capturing Photos"},{"location":"Image/#image-capture","text":"To ensure the light painting will be visible, the surrounding lights should be turned off to increase contrast, while using long exposure photography to capture each frame. Set the camera ISO to 100 to reduce the graininess of images. To reduce light entering the camera and increasing contrast, set the aperture to f/22. Ensure the entire image is in focus by using ensuring the depth of field is at its maximum depending on your camera. This can be calculated here . Mount the camera on a tripod with an external camera remote to avoid camera shake. Set the camera on \u201ctime\u201d mode, so that one press of the shutter would open the aperture and another would close the aperture. As the duration of each frame varies, this would provide a simple error-prood method. Before taking photos, focus the camera on the origin of the end-effector using autofocus and then put on manual focus for the rest of the images. With the code provided, Franka Emika would signal when to press the shutter by wiggling before and after each frame begins.","title":"Image Capture"},{"location":"about/","text":"Team Harvey Upton Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot, compiled images into animatioon & video editing. Tom\u00e1\u0161 K\u0148aze Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot David Prior Hope Image capturing of images with long exposure photography, researched intor implementing error detection. Pauline Ng Design, manufacture and assemble controllable light painting module and docking mechanism. Luke Hillery Generate simulation of motion planning in Gazebo and Rviz software, including light painting simulation for testing, assisted with animation generaion sequence. Higor Alves De Freitas Generated and processed coordinates for each frame of animation sequence by creating grasshoper definition to produce coordinate sets from any configuration of the butterfly mesh, processed coordinate data.","title":"Team"},{"location":"about/#team","text":"Harvey Upton Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot, compiled images into animatioon & video editing. Tom\u00e1\u0161 K\u0148aze Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot David Prior Hope Image capturing of images with long exposure photography, researched intor implementing error detection. Pauline Ng Design, manufacture and assemble controllable light painting module and docking mechanism. Luke Hillery Generate simulation of motion planning in Gazebo and Rviz software, including light painting simulation for testing, assisted with animation generaion sequence. Higor Alves De Freitas Generated and processed coordinates for each frame of animation sequence by creating grasshoper definition to produce coordinate sets from any configuration of the butterfly mesh, processed coordinate data.","title":"Team"},{"location":"animationcreation/","text":"Animation Creation The butterfly model was simplified to a low-poly model since it was expected that a large number of frames would be needed to form a smooth animation. I was also intended that the focus was on the animation itself and not on the model. As the project was limited in terms of time with the robot, a simplified outline presented the most opportunity to iterate and increase robotic complexity. Ie. A low fidelity model reduces the amount of time necessary for the robot to complete each frame. Using slow-motion footage of butterflies in flight, one model for each frame of the animation was created in Rhinoceros 3D . The animation was simplified to 11 frames. The angles of the butterfly wings about the x-axis and the z-axis are approximately sinusoidal. A subtle body wiggle was also added, which is a small, random rotation of the two body triangles about the y-axis. (Angles in reference to this video axes). *NOTE Grasshopper is a plugin included in Rhino 6, which has built-in functions called \u201ccomponents\u201d to allow for easy data manipulation. Butterfly Animation To create your own butterfly animation, follow these steps below: 1. Open Rhino 6. 2. Draw an equilateral triangle on the XY plane, with one vertex on the origin. 3. With the mirror command, mirror the triangle through the XZ plane to form the buttefly wings. 4. Draw an isoscles triangle on the XZ plane with the vertex corresponding to the smallest angle to the origin. 5. Using the mirror command, mirror the triangle through the XY plane to form the butterfly body. 6. \"CTRL +G\" to group all four triangles. 7. Copy the group to get 11 instances in total where 1 is for each frame. 8. Isolate each group to its own seperate layer by using the 'Change Layer Command'. 9. Rotate the wings about the X and Z axes according with the reference of a buterfly flight footage incrementally over 11 frames. 10. Open the Grasshopper plugin. 11. Open the stl_file_to_edges_and_coordinates.gh file in Grasshopper. 12. Right click on the first block in each frame and click 'Set Multiple Meshes'. 13. Select the group in the associated layer. Ie. Layer 1 group for frame 1. 14. Wait for the algorithm to run. 15. Right click on the last block for each frame and select 'Copy Data Only'. 16. Paste the data in your preferred text editor in the correct frame order. 17. Replace all curly brackets with square brackets. 18. Format the data so that it looks like the list in coordinate_list_about-origin.py The stl_file_to_edges_and_coordinates.gh file can be found here The coordinate_list_about-origin.py file can be found here General Animation These are the general steps which can be repeated to output the same result for each frame in Grasshopper of your animation of choice: 1. Deconstruct a single image into vertices, faces, colours and normals using the 'deconstruct mesh' components. 2. The output will be a list of a list, where each vertex is in a XYZ coordinate. 3. Remove any repeated centre vertices by putting the lists through the 'create set' component. 4. Extract each sublist using the 'list item' component. 5. Reorded the list for the robot to move from coordinate to coordinate without retracing the same path twice. *NOTE This is important to avoid any brighter spots in the animation frames which would be seen as an anomaly. 6. The list of coordinates will be in curly brackets, replace them with square brackets. This general method allows for the flexibility of changing the butterfly animation frame by frame without the need to manually change each coordinate. This would also allow for more complexity to be added easily on the butterfly models (ie. higher poly models), at the cost of a longer per-frame drawing time. The Grasshopper definition shown in figure below is robust enough to deal with having curved lines, and would output these as a higher number of interpolated points to form the curves.","title":"Creating Animation"},{"location":"animationcreation/#animation-creation","text":"The butterfly model was simplified to a low-poly model since it was expected that a large number of frames would be needed to form a smooth animation. I was also intended that the focus was on the animation itself and not on the model. As the project was limited in terms of time with the robot, a simplified outline presented the most opportunity to iterate and increase robotic complexity. Ie. A low fidelity model reduces the amount of time necessary for the robot to complete each frame. Using slow-motion footage of butterflies in flight, one model for each frame of the animation was created in Rhinoceros 3D . The animation was simplified to 11 frames. The angles of the butterfly wings about the x-axis and the z-axis are approximately sinusoidal. A subtle body wiggle was also added, which is a small, random rotation of the two body triangles about the y-axis. (Angles in reference to this video axes). *NOTE Grasshopper is a plugin included in Rhino 6, which has built-in functions called \u201ccomponents\u201d to allow for easy data manipulation.","title":"Animation Creation"},{"location":"animationcreation/#butterfly-animation","text":"To create your own butterfly animation, follow these steps below: 1. Open Rhino 6. 2. Draw an equilateral triangle on the XY plane, with one vertex on the origin. 3. With the mirror command, mirror the triangle through the XZ plane to form the buttefly wings. 4. Draw an isoscles triangle on the XZ plane with the vertex corresponding to the smallest angle to the origin. 5. Using the mirror command, mirror the triangle through the XY plane to form the butterfly body. 6. \"CTRL +G\" to group all four triangles. 7. Copy the group to get 11 instances in total where 1 is for each frame. 8. Isolate each group to its own seperate layer by using the 'Change Layer Command'. 9. Rotate the wings about the X and Z axes according with the reference of a buterfly flight footage incrementally over 11 frames. 10. Open the Grasshopper plugin. 11. Open the stl_file_to_edges_and_coordinates.gh file in Grasshopper. 12. Right click on the first block in each frame and click 'Set Multiple Meshes'. 13. Select the group in the associated layer. Ie. Layer 1 group for frame 1. 14. Wait for the algorithm to run. 15. Right click on the last block for each frame and select 'Copy Data Only'. 16. Paste the data in your preferred text editor in the correct frame order. 17. Replace all curly brackets with square brackets. 18. Format the data so that it looks like the list in coordinate_list_about-origin.py The stl_file_to_edges_and_coordinates.gh file can be found here The coordinate_list_about-origin.py file can be found here","title":"Butterfly Animation"},{"location":"animationcreation/#general-animation","text":"These are the general steps which can be repeated to output the same result for each frame in Grasshopper of your animation of choice: 1. Deconstruct a single image into vertices, faces, colours and normals using the 'deconstruct mesh' components. 2. The output will be a list of a list, where each vertex is in a XYZ coordinate. 3. Remove any repeated centre vertices by putting the lists through the 'create set' component. 4. Extract each sublist using the 'list item' component. 5. Reorded the list for the robot to move from coordinate to coordinate without retracing the same path twice. *NOTE This is important to avoid any brighter spots in the animation frames which would be seen as an anomaly. 6. The list of coordinates will be in curly brackets, replace them with square brackets. This general method allows for the flexibility of changing the butterfly animation frame by frame without the need to manually change each coordinate. This would also allow for more complexity to be added easily on the butterfly models (ie. higher poly models), at the cost of a longer per-frame drawing time. The Grasshopper definition shown in figure below is robust enough to deal with having curved lines, and would output these as a higher number of interpolated points to form the curves.","title":"General Animation"},{"location":"betatest/","text":"Lo-Fi Beta Testing To validate the concept, a beta test was first conducted without the robotic arm. In this experiment, a human, an iPhone torch, a template, a stand, a camera and a tripod were used. The template was rotated at approximately 20 degree intervals through 180 degrees, pivoting on the stand. At each rotation, a long exposure image was taken on the camera and the human moved the light around the frame. Once this sequence was complete, the images were manually compiled in a video editing software to create the beta test animation. Steps Mount camera on tripod & template about stand Open camera shutter Trace image with torch Close camera shutter Rotate template approximately 20\u00b0 about stand Repeat steps 2 to 5 Import images Compile images","title":"Lo-Fi Development"},{"location":"betatest/#lo-fi-beta-testing","text":"To validate the concept, a beta test was first conducted without the robotic arm. In this experiment, a human, an iPhone torch, a template, a stand, a camera and a tripod were used. The template was rotated at approximately 20 degree intervals through 180 degrees, pivoting on the stand. At each rotation, a long exposure image was taken on the camera and the human moved the light around the frame. Once this sequence was complete, the images were manually compiled in a video editing software to create the beta test animation.","title":"Lo-Fi Beta Testing"},{"location":"betatest/#steps","text":"Mount camera on tripod & template about stand Open camera shutter Trace image with torch Close camera shutter Rotate template approximately 20\u00b0 about stand Repeat steps 2 to 5 Import images Compile images","title":"Steps"},{"location":"evaluation/","text":"Outcomes Error Detection The chances of error were significantly mitigated by ensuring the motion plan to only be within the workspace of the robot, therefore reducing chances of motion planning error due to collision or being out of the task space. However, error detection and recovery could have been built into the code by using Frankx, which is a high-level motion planning library for Franka Emika Panda. Frankx is a python wrapper for Libfranka (which only works in C++), using Reflexes as a trajectory generator, Eigen for transformation calculations and pybind11 for Python bindings. Once installed, the command robot.recover_from_errors() allows the robot to automatically recover from an error by resetting the robot. Missed Opportunities The following section includes missed opportunities that could have been implemented given a longer project time period. Automating Image Capture This could have been implemented through designing a custom shutter with a large surface area connected to the camera. The shutter could then be pressed by the robot in-between frames to open and close the aperture before and at the end of the motion. Another option could be to use an infrared sensor which the robot would need to swipe over, this would require less advanced hardware and could prevent damage of the robot due to collision. The robot operating sequence would in turn change with this setup, without the need for a wiggle indication and to shift the priority towards turning the light on and off with the gripper becoming to prevent a streak in each frame. Image Compilation This was challenging using the hardware available since the images were stored in an SD card. A way for the stop motion animation to be automatically compiled is by using a Raspberry Pi with a camera module. In the code, each image could be appended to a sequence of images and then exported as a video. This would then be able to automatically compile an animation at the end of the robot operating sequence. Speed Control Speed control could have been implemented. The intensity of light in an image was proportional to the time spent in each position (ie. speed, s); brightness of the torch, bt; and angle of motion relative to the orbiting camera angle, a (varying sinusoidally, from maximum when a = 90 degrees, to minimum when a = 0).Image brightness, B = (k bt + x sin(a)) / s Where k and x are constants. Given the above equation, setting B to a constant and defining the pose and position of the camera would mean that the speed could be calculated to maintain a constant brightness in the light trail. Ultimately, however, the images proved to be aesthetic without this, thus it was considered a redundancy. Picking up the torch was executed by directing the robot arm to the correct position and then allowing the torch to snap on using magnets. This assumes that the resting position and pose of the torch is always the same. An opportunity could have been to include computer vision to locate the current position and pose of the torch and then generate a motion plan to pick up the torch. This would have been a more robust method, however, would drastically increase the complexity of the robot by including image processing. Potentially the image processing could be done using a Raspberry Pi camera (same as the one for frame photography) which could provide data to create a motion plan to pick up the torch.","title":"Future Development"},{"location":"evaluation/#outcomes","text":"","title":"Outcomes"},{"location":"evaluation/#error-detection","text":"The chances of error were significantly mitigated by ensuring the motion plan to only be within the workspace of the robot, therefore reducing chances of motion planning error due to collision or being out of the task space. However, error detection and recovery could have been built into the code by using Frankx, which is a high-level motion planning library for Franka Emika Panda. Frankx is a python wrapper for Libfranka (which only works in C++), using Reflexes as a trajectory generator, Eigen for transformation calculations and pybind11 for Python bindings. Once installed, the command robot.recover_from_errors() allows the robot to automatically recover from an error by resetting the robot.","title":"Error Detection"},{"location":"evaluation/#missed-opportunities","text":"The following section includes missed opportunities that could have been implemented given a longer project time period. Automating Image Capture This could have been implemented through designing a custom shutter with a large surface area connected to the camera. The shutter could then be pressed by the robot in-between frames to open and close the aperture before and at the end of the motion. Another option could be to use an infrared sensor which the robot would need to swipe over, this would require less advanced hardware and could prevent damage of the robot due to collision. The robot operating sequence would in turn change with this setup, without the need for a wiggle indication and to shift the priority towards turning the light on and off with the gripper becoming to prevent a streak in each frame. Image Compilation This was challenging using the hardware available since the images were stored in an SD card. A way for the stop motion animation to be automatically compiled is by using a Raspberry Pi with a camera module. In the code, each image could be appended to a sequence of images and then exported as a video. This would then be able to automatically compile an animation at the end of the robot operating sequence. Speed Control Speed control could have been implemented. The intensity of light in an image was proportional to the time spent in each position (ie. speed, s); brightness of the torch, bt; and angle of motion relative to the orbiting camera angle, a (varying sinusoidally, from maximum when a = 90 degrees, to minimum when a = 0).Image brightness, B = (k bt + x sin(a)) / s Where k and x are constants. Given the above equation, setting B to a constant and defining the pose and position of the camera would mean that the speed could be calculated to maintain a constant brightness in the light trail. Ultimately, however, the images proved to be aesthetic without this, thus it was considered a redundancy. Picking up the torch was executed by directing the robot arm to the correct position and then allowing the torch to snap on using magnets. This assumes that the resting position and pose of the torch is always the same. An opportunity could have been to include computer vision to locate the current position and pose of the torch and then generate a motion plan to pick up the torch. This would have been a more robust method, however, would drastically increase the complexity of the robot by including image processing. Potentially the image processing could be done using a Raspberry Pi camera (same as the one for frame photography) which could provide data to create a motion plan to pick up the torch.","title":"Missed Opportunities"},{"location":"fail/","text":"","title":"Failed Attempts"},{"location":"hardware/","text":"Franka Emika Software Software updates can be found at: http://support.franka.de/ The software versions currently used in the robotics lab are: Franka Firmware 1.0.9 Supports libfranka < 0.2.0 ros-kinetic-libfranka 0.1.0 Current ROS version is 0.2.0 The lab only supports libfranka 0.1.0 which is currently unavailable from apt install . Do NOT uninstall ROS or libfranka on workstations which already have it installed . Getting started This section details how to connect the robot and test the setup by using FCI to read the current state. Operating the robot Before going further though, here are a few safety considerations. Always check the following things before powering on the robot: Make sure that the arm has been mounted on a stable base and cannot topple over, even when performing fast motions or abrupt stops. Caution Only tabletop mounting is supported, i.e. the Arm must be mounted perpendicular to the ground! Other mountings will void your warranty and might cause damage to the robot! Ensure that the cable connecting Arm and Control is firmly attached on both sides. Connect the external activation device to Arm\u2019s base and keep it next to you in order to be able to stop the robot at any time. Hint Activating the external activation device will disconnect the Arm from Control. The joint motor controllers will then hold their current position. The external activation device is not an emergency stop! This list is non-exhaustive! The manual delivered with your robot contains a chapter dedicated to safety. Please read it carefully and follow the instructions. Important The workstation PC which commands your robot using the FCI must always be connected to the LAN port of Control (shop floor network) and not to the LAN port of the Arm (robot network). Setting up the network Good network performance is crucial when controlling the robot using FCI. Therefore it is strongly recommended to use a direct connection between the workstation PC and Panda\u2019s Control. This section describes how to configure your network for this use case. _images/control.png Use Control\u2019s LAN port when controlling the robot through FCI. Do not connect to the port in Arm\u2019s base. The Control and your workstation must be configured to appear on the same network. Simplest way to achieve that is to use static IP addresses. Any two addresses on the same network would work, but the following values will be used for the purpose of this tutorial: Workstation PC Control Address 172.16.0.1 172.16.0.2 Netmask 24 24 The Control\u2019s address (172.16.0.2) is called in the following chapters. Hint With this network configuration, Desk can be accessed via https:// , although you will see a certificate warning in your browser. The configuration process consists of two steps: Configuring Control\u2019s network settings. Configuring your workstation\u2019s network settings. Control network configuration For this step, the robot needs to be installed and tested. Please read through the documents shipped with your robot and follow the setup instructions before continuing any further! The Control\u2019s network can be configured in the administrator\u2019s interface. For the duration of this step you can connect to the robot through the port in the robot\u2019s base. For details, consult the Connecting a user interface device section in the manual delivered with your robot. _images/accessing-admin.png Accessing the administrator\u2019s interface through Desk. To set up a static address, enter the following values in the Network section: _images/control-static-ip.png Setting a static IP for the Control\u2019s LAN port (Shop Floor network). DHCP Client option is deselected. Press Apply. After the settings are successfully applied, connect your workstation\u2019s LAN port to the robot\u2019s control unit. Linux workstation network configuration This section describes how to set up a static IP address on Ubuntu 16.04 using the GUI. Follow the official Ubuntu guide if you prefer to use the command line. Caution The following steps will modify your network settings. If in doubt, contact your network\u2019s administrator. First, go to Network Connection widget. Select the wired connection you will be using and click edit. _images/edit-connections.png Edit the connection in the Ethernet section. Next, click on the IPv4 settings tab, set the method to Manual, and enter the following values: _images/static-ip-ubuntu.png Setting a static IP for the Workstation PC. Method is set to Manual. Hint This step will disable DHCP, which means you will no longer obtain an address when connecting to a DHCP server, like the one in Arm\u2019s base. When you no longer use FCI, you can change the Method back to Automatic (DHCP). Save the changes, and close the Network Connection window. Click on the connection name from the drop down menu. It should now be possible to connect to the robot from your workstation. To verify this, perform the Network bandwidth, delay and jitter test. From now on, you can also access Desk through this address in your browser. Windows workstation network configuration Setup a static IP address on the Windows workstation. Therefore, open Control Panel and go to Network and Internet > Network and Sharing Center > Change adapter settings. Right-click the network adapter and open Properties. Use the same example address and netmask as in the Linux workstation network configuration. Verifying the connection The previous section described how to specify the IP address of the Control\u2019s LAN port. In the following sections that address is referred to as . In order to verify that everything is correctly set up, run the echo_robot_state example from libfranka. If you decided to install franka_ros and libfranka from the ROS repository, you can instead read the instructions for visualizing the robot in ros . Change to the build directory of libfranka and execute the example: Linux: ./examples/echo_robot_state Windows: cd /path/to/libfranka/build/examples/ echo_robot_state.exe Hint Before executing libfranka programms, make sure that the executables are able to find their runtime libraries. On Windows, the easiest way is to copy the needed libraries into the same directory as the executable. The program will print the current state of the robot to the console and terminate after a few iterations. The fields are explained in the libfranka API documentation. Example output: Hint If an error occurs at this point, perform the ping test and ensure that the robot\u2019s fail-safe safety locking system is opened. Further information are provided in the manual shipped with the robot.","title":"Hardware"},{"location":"hardware/#franka-emika-software","text":"Software updates can be found at: http://support.franka.de/ The software versions currently used in the robotics lab are: Franka Firmware 1.0.9 Supports libfranka < 0.2.0 ros-kinetic-libfranka 0.1.0 Current ROS version is 0.2.0 The lab only supports libfranka 0.1.0 which is currently unavailable from apt install . Do NOT uninstall ROS or libfranka on workstations which already have it installed .","title":"Franka Emika Software"},{"location":"hardware/#getting-started","text":"This section details how to connect the robot and test the setup by using FCI to read the current state.","title":"Getting started"},{"location":"hardware/#operating-the-robot","text":"Before going further though, here are a few safety considerations. Always check the following things before powering on the robot: Make sure that the arm has been mounted on a stable base and cannot topple over, even when performing fast motions or abrupt stops. Caution Only tabletop mounting is supported, i.e. the Arm must be mounted perpendicular to the ground! Other mountings will void your warranty and might cause damage to the robot! Ensure that the cable connecting Arm and Control is firmly attached on both sides. Connect the external activation device to Arm\u2019s base and keep it next to you in order to be able to stop the robot at any time. Hint Activating the external activation device will disconnect the Arm from Control. The joint motor controllers will then hold their current position. The external activation device is not an emergency stop! This list is non-exhaustive! The manual delivered with your robot contains a chapter dedicated to safety. Please read it carefully and follow the instructions. Important The workstation PC which commands your robot using the FCI must always be connected to the LAN port of Control (shop floor network) and not to the LAN port of the Arm (robot network). Setting up the network Good network performance is crucial when controlling the robot using FCI. Therefore it is strongly recommended to use a direct connection between the workstation PC and Panda\u2019s Control. This section describes how to configure your network for this use case. _images/control.png Use Control\u2019s LAN port when controlling the robot through FCI. Do not connect to the port in Arm\u2019s base. The Control and your workstation must be configured to appear on the same network. Simplest way to achieve that is to use static IP addresses. Any two addresses on the same network would work, but the following values will be used for the purpose of this tutorial: Workstation PC Control Address 172.16.0.1 172.16.0.2 Netmask 24 24 The Control\u2019s address (172.16.0.2) is called in the following chapters. Hint With this network configuration, Desk can be accessed via https:// , although you will see a certificate warning in your browser. The configuration process consists of two steps: Configuring Control\u2019s network settings. Configuring your workstation\u2019s network settings. Control network configuration For this step, the robot needs to be installed and tested. Please read through the documents shipped with your robot and follow the setup instructions before continuing any further! The Control\u2019s network can be configured in the administrator\u2019s interface. For the duration of this step you can connect to the robot through the port in the robot\u2019s base. For details, consult the Connecting a user interface device section in the manual delivered with your robot. _images/accessing-admin.png Accessing the administrator\u2019s interface through Desk. To set up a static address, enter the following values in the Network section: _images/control-static-ip.png Setting a static IP for the Control\u2019s LAN port (Shop Floor network). DHCP Client option is deselected. Press Apply. After the settings are successfully applied, connect your workstation\u2019s LAN port to the robot\u2019s control unit. Linux workstation network configuration This section describes how to set up a static IP address on Ubuntu 16.04 using the GUI. Follow the official Ubuntu guide if you prefer to use the command line. Caution The following steps will modify your network settings. If in doubt, contact your network\u2019s administrator. First, go to Network Connection widget. Select the wired connection you will be using and click edit. _images/edit-connections.png Edit the connection in the Ethernet section. Next, click on the IPv4 settings tab, set the method to Manual, and enter the following values: _images/static-ip-ubuntu.png Setting a static IP for the Workstation PC. Method is set to Manual. Hint This step will disable DHCP, which means you will no longer obtain an address when connecting to a DHCP server, like the one in Arm\u2019s base. When you no longer use FCI, you can change the Method back to Automatic (DHCP). Save the changes, and close the Network Connection window. Click on the connection name from the drop down menu. It should now be possible to connect to the robot from your workstation. To verify this, perform the Network bandwidth, delay and jitter test. From now on, you can also access Desk through this address in your browser. Windows workstation network configuration Setup a static IP address on the Windows workstation. Therefore, open Control Panel and go to Network and Internet > Network and Sharing Center > Change adapter settings. Right-click the network adapter and open Properties. Use the same example address and netmask as in the Linux workstation network configuration. Verifying the connection The previous section described how to specify the IP address of the Control\u2019s LAN port. In the following sections that address is referred to as . In order to verify that everything is correctly set up, run the echo_robot_state example from libfranka. If you decided to install franka_ros and libfranka from the ROS repository, you can instead read the instructions for visualizing the robot in ros . Change to the build directory of libfranka and execute the example: Linux: ./examples/echo_robot_state Windows: cd /path/to/libfranka/build/examples/ echo_robot_state.exe Hint Before executing libfranka programms, make sure that the executables are able to find their runtime libraries. On Windows, the easiest way is to copy the needed libraries into the same directory as the executable. The program will print the current state of the robot to the console and terminate after a few iterations. The fields are explained in the libfranka API documentation. Example output: Hint If an error occurs at this point, perform the ping test and ensure that the robot\u2019s fail-safe safety locking system is opened. Further information are provided in the manual shipped with the robot.","title":"Operating the robot"},{"location":"implementation/","text":"Implementation on Panda Setting Up Franka Emika First, release the brakes by making sure the external activation device (EAD) is pressed down. Then, in the controller web interface, click to open brakes. A clicking sound will be heard indicating the EAD button needs to be released. After releasing the EAD button, the LED light will turn white, ready to be used. Controlling Franka & ROS With libfranka and franka_ros installed in the real-time Linux kernel, the following steps were carried out before launching the pre-programmed code. Open terminal and type cd franka_ws Then, type source /devel/setup.bash Split the terminal into 5 terminals. In the following order, type in these commands in the respective terminals. \u2022 Terminal 1: roscore \u2022 Terminal 2: roslaunch franka_control franka_control.launch robot_ip:=192.168.0.88 load_gripper:=true \u2022 Terminal 3: roslaunch panda_moveit_config panda_moveit. launchcontroller:=position \u2022 Terminal 4: roslaunch panda_moveit_config moveit_rviz.launch \u2022 Terminal 5: python \u2018NAMEOFCODE\u2019.py NOTE: to change the \u2018NAMEOFCODE\u2019 as the name of python file to be executed.","title":"Implementation"},{"location":"implementation/#implementation-on-panda","text":"","title":"Implementation on Panda"},{"location":"implementation/#setting-up-franka-emika","text":"First, release the brakes by making sure the external activation device (EAD) is pressed down. Then, in the controller web interface, click to open brakes. A clicking sound will be heard indicating the EAD button needs to be released. After releasing the EAD button, the LED light will turn white, ready to be used.","title":"Setting Up Franka Emika"},{"location":"implementation/#controlling-franka-ros","text":"With libfranka and franka_ros installed in the real-time Linux kernel, the following steps were carried out before launching the pre-programmed code. Open terminal and type cd franka_ws Then, type source /devel/setup.bash Split the terminal into 5 terminals. In the following order, type in these commands in the respective terminals. \u2022 Terminal 1: roscore \u2022 Terminal 2: roslaunch franka_control franka_control.launch robot_ip:=192.168.0.88 load_gripper:=true \u2022 Terminal 3: roslaunch panda_moveit_config panda_moveit. launchcontroller:=position \u2022 Terminal 4: roslaunch panda_moveit_config moveit_rviz.launch \u2022 Terminal 5: python \u2018NAMEOFCODE\u2019.py NOTE: to change the \u2018NAMEOFCODE\u2019 as the name of python file to be executed.","title":"Controlling Franka &amp; ROS"},{"location":"intro/","text":"Introduction Drawing inspiration from light painting images, this project entails programming the Franka Emika robotic arm to draw several still images of a butterfly with light to produce a stop motion animation. Lo-Fi Beta Testing To validate the concept, a beta test was first conducted without the robotic arm. In this experiment, a human, an iPhone torch, a template, a stand, a camera and a tripod were used. The template was rotated at approximately 20 degree intervals through 180 degrees, pivoting on the stand. At each rotation, a long exposure image was taken on the camera and the human moved the light around the frame. Once this sequence was complete, the images were manually compiled in a video editing software to create the beta test animation.","title":"Intro"},{"location":"intro/#introduction","text":"Drawing inspiration from light painting images, this project entails programming the Franka Emika robotic arm to draw several still images of a butterfly with light to produce a stop motion animation.","title":"Introduction"},{"location":"intro/#lo-fi-beta-testing","text":"To validate the concept, a beta test was first conducted without the robotic arm. In this experiment, a human, an iPhone torch, a template, a stand, a camera and a tripod were used. The template was rotated at approximately 20 degree intervals through 180 degrees, pivoting on the stand. At each rotation, a long exposure image was taken on the camera and the human moved the light around the frame. Once this sequence was complete, the images were manually compiled in a video editing software to create the beta test animation.","title":"Lo-Fi Beta Testing"},{"location":"lightsource/","text":"Light Source A custom light torch was designed and manufactured to comply towards the dimensions of the Franka Emika end effector gripper. The STL file of the light stick can be found here and be FDM printed on 0.2mm layer height at 30% infill. The light unit was designed as a hexagonal shape to be held securely on the end effector with a cone tip to direct the light source towards the centre of the unit. A second iteration was made to allow for the unit to be picked up automatically along with a single pole single throw (SPST) switch attached for the light source to stay on when the end effector gripper is closed. Attachment To add in the motion of the light stick being picked up, a magnetic strip was designed to be permanently attached on one end of the end effector. Magnets of the opposite polarity were attached on the main light unit to ensure it remains attached on the end effector when the gripper is opened. Circuitry The internal circuitry of the light unit consists of two 1.5V AA Batteries, connected to a SPST switch and an LED. This would ensure that when the end effector gripper is closed, the light is turned on and vice versa when the gripper is open. This would also remove unwanted bright spots and allow for the possibility of continuous images within the same frame.","title":"Panda Torch"},{"location":"lightsource/#light-source","text":"A custom light torch was designed and manufactured to comply towards the dimensions of the Franka Emika end effector gripper. The STL file of the light stick can be found here and be FDM printed on 0.2mm layer height at 30% infill. The light unit was designed as a hexagonal shape to be held securely on the end effector with a cone tip to direct the light source towards the centre of the unit. A second iteration was made to allow for the unit to be picked up automatically along with a single pole single throw (SPST) switch attached for the light source to stay on when the end effector gripper is closed.","title":"Light Source"},{"location":"lightsource/#attachment","text":"To add in the motion of the light stick being picked up, a magnetic strip was designed to be permanently attached on one end of the end effector. Magnets of the opposite polarity were attached on the main light unit to ensure it remains attached on the end effector when the gripper is opened.","title":"Attachment"},{"location":"lightsource/#circuitry","text":"The internal circuitry of the light unit consists of two 1.5V AA Batteries, connected to a SPST switch and an LED. This would ensure that when the end effector gripper is closed, the light is turned on and vice versa when the gripper is open. This would also remove unwanted bright spots and allow for the possibility of continuous images within the same frame.","title":"Circuitry"},{"location":"methodoverview/","text":"Method Overview This project was implemented and segmented into 6 main steps 1. Animation Sequencing The model sequence of the butterfly in flight was generated using Rhinoceros3D and the Grasshopper plugin, iteratvely altering a low-poly mesh to give the effect of a butterfly flapping its wings. 2. Coordinate Generation The frame sequence was then treated as individual frames, with the coordinates of each one being extracted and processed to eliminate duplicae points. The coordinates were organised to prevent duplicate paths. 3. Motion Planning A motion plan was then generated for each frame using the coordinate list created in the previous step. Code to iterate over the list of frames and plan each motion was developed. This was simulated using RViz and Gazebo. 4. Simulation Once simulated successfully, the code was implemented using Franka Emika\u2019s Panda robotic arm. 5. Implementation An end-effector attachment containing a controllable LED was simultaneously developed and manufactured to facilitate light painting 6. Image Capture Once the code successfully ran repeatedly on the Panda robot, long-exposure photgraphs were captured of the motion. These photographs were then collated and displayed in sequence to create an animation of a butterfly in flight.","title":"Methodoverview"},{"location":"methodoverview/#method-overview","text":"This project was implemented and segmented into 6 main steps","title":"Method Overview"},{"location":"methodoverview/#1-animation-sequencing","text":"The model sequence of the butterfly in flight was generated using Rhinoceros3D and the Grasshopper plugin, iteratvely altering a low-poly mesh to give the effect of a butterfly flapping its wings.","title":"1. Animation Sequencing"},{"location":"methodoverview/#2-coordinate-generation","text":"The frame sequence was then treated as individual frames, with the coordinates of each one being extracted and processed to eliminate duplicae points. The coordinates were organised to prevent duplicate paths.","title":"2. Coordinate Generation"},{"location":"methodoverview/#3-motion-planning","text":"A motion plan was then generated for each frame using the coordinate list created in the previous step. Code to iterate over the list of frames and plan each motion was developed. This was simulated using RViz and Gazebo.","title":"3. Motion Planning"},{"location":"methodoverview/#4-simulation","text":"Once simulated successfully, the code was implemented using Franka Emika\u2019s Panda robotic arm.","title":"4. Simulation"},{"location":"methodoverview/#5-implementation","text":"An end-effector attachment containing a controllable LED was simultaneously developed and manufactured to facilitate light painting","title":"5. Implementation"},{"location":"methodoverview/#6-image-capture","text":"Once the code successfully ran repeatedly on the Panda robot, long-exposure photgraphs were captured of the motion. These photographs were then collated and displayed in sequence to create an animation of a butterfly in flight.","title":"6. Image Capture"},{"location":"photo/","text":"","title":"Photos"},{"location":"projectdevelopment/","text":"Project Development This section of the documentation consists of the development from animation creation to image capturing Method Overview This project was implemented and segmented into 6 main steps 1. Animation Sequencing The model sequence of the butterfly in flight was generated using Rhinoceros3D and the Grasshopper plugin, iteratvely altering a low-poly mesh to give the effect of a butterfly flapping its wings. 2. Coordinate Generation The frame sequence was then treated as individual frames, with the coordinates of each one being extracted and processed to eliminate duplicae points. The coordinates were organised to prevent duplicate paths. 3. Motion Planning A motion plan was then generated for each frame using the coordinate list created in the previous step. Code to iterate over the list of frames and plan each motion was developed. This was simulated using RViz and Gazebo. 4. Simulation Once simulated successfully, the code was implemented using Franka Emika\u2019s Panda robotic arm. 5. Implementation An end-effector attachment containing a controllable LED was simultaneously developed and manufactured to facilitate light painting 6. Image Capture Once the code successfully ran repeatedly on the Panda robot, long-exposure photgraphs were captured of the motion. These photographs were then collated and displayed in sequence to create an animation of a butterfly in flight. Animation Creation The butterfly model was simplified to a low-poly model since it was expected that a large number of frames would be needed to form a smooth animation. I was also intended that the focus was on the animation itself and not on the model. As the project was limited in terms of time with the robot, a simplified outline presented the most opportunity to iterate and increase robotic complexity. I.e. A low fidelity model reduces the amount of time necessary for the robot to complete each frame. Using slow-motion footage of butterflies in flight, one model for each frame of the animation was created in Rhinoceros 3D . The animation was simplified to 11 frames. The angles of the butterfly wings about the x-axis and the z-axis are approximately sinusoidal. A subtle body wiggle was also added, which is a small, random rotation of the two body triangles about the y-axis. (Angles in reference to Figure B axes). NOTE: Note: Grasshopper is a plugin included in Rhino 6, which has built-in functions called \u201ccomponents\u201d to allow for easy data manipulation. These are the steps which can be repeated to output the same result for each frame in Grasshopper: Each of the 4 triangles which form a butterfly were deconstructed into their vertices, faces, colours and normals using the \u201cdeconstruct mesh\u201d components. This outputs as a list of lists; a list of 4 triangles and each triangle having 3 vertices, where each vertex is an xyz coordinate. The repeated centre vertices were removed by putting the lists through the \u201ccreate set\u201d component. Each sublist was extracted using a \u201clist item\u201d components. Reorder the list in such a way that the robot can move from coordinate to coordinate without retracing the same path twice. NOTE: This is important in order to avoid any brighter spots in the animation frames which would be seen as an anomaly. The result is a list of coordinates in curly brackets. Simple data manipulation was then carried out in a text editor to put this in the required python form which uses square brackets (replace function in Wordpad, replacing \u201c{\u201c and \u201c}\u201d for \u201c[\u201c and \u201c]\u201d respectively. This method allows for the flexibility of changing the butterfly animation frame by frame without the need to manually change each coordinate. This would also allow for more complexity to be added easily on the butterfly models (ie. higher poly models), at the cost of a longer per-frame drawing time. The Grasshopper definition shown in Figure C is robust enough to deal with having curved lines, and would output these as a higher number of interpolated points to form the curves. Given more time with the robot, this change could be implemented. Motion Planning Harvey Simulation Motion Planning Simulation Before real-life implementation of the code on the Panda robot, the entire task was tested using RViz and Gazebo simulations. This allowed for all poses and time functions to be tested without the risk of damaging the actual robot. It also made it easier to collaborate on and iteratively improve the code as it was not necessary for each team member to individually access the physical robot. The Panda model was loaded, spawned and placed into the Gazebo environment, then controlled with the Python code written by the group members. Light Painting Simulation By tracing the position of the end effector in RViz, it allowed the path of the attached light-painting module to be recreated in the RViz viewport. This method proved to be much more efficient than camera implementation within Gazebo, which required additional plugins (such as a camera sensor plugin to publish the live image to an image topic and LinkPlot3DPlugin to achieve isolated link tracing) and produced a noisier output due to the shaking exhibited within Gazebo simulations. This effectively generated a digital model of the light painting that the Panda robot would recreate in real life. To initialize the light tracing simulation in RViz, go to: Motion Planning Planned Path Links Check Panda_Hand[_] Check Show Trail [_] Light Source A custom light torch was designed and manufactured to comply towards the dimensions of the Franka Emika end effector gripper. The STL file of the light stick can be found here and be FDM printed on 0.2mm layer height at 30% infill. The light unit was designed as a hexagonal shape to be held securely on the end effector with a cone tip to direct the light source towards the centre of the unit. A second iteration was made to allow for the unit to be picked up automatically along with a single pole single throw (SPST) switch attached for the light source to stay on when the end effector gripper is closed. Attachment To add in the motion of the light stick being picked up, a magnetic strip was designed to be permanently attached on one end of the end effector. Magnets of the opposite polarity were attached on the main light unite to ensure it remains attached on the end effector when the gripper is opened. Circuitry The internal circuitry of the light unit consists of two 1.5V AA Batteries, connected to a SPST switch and an LED. This would ensure that when the end effector gripper is closed, the light is turned on and vice versa when the gripper is open. This would also remove unwanted bright spots and allow for the possibility of continuous images within the same frame. Implementation on Panda Setting Up Franka Emika First, release the brakes by making sure the external activation device (EAD) is pressed down. Then, in the controller web interface, click to open brakes. A clicking sound will be heard indicating the EAD button needs to be released. After releasing the EAD button, the LED light will turn white, ready to be used. Controlling Franka & ROS With libfranka and franka_ros installed in the real-time Linux kernel, the following steps were carried out before launching the pre-programmed code. Open terminal and type cd franka_ws Then, type source /devel/setup.bash Split the terminal into 5 terminals. In the following order, type in these commands in the respective terminals. \u2022 Terminal 1: roscore \u2022 Terminal 2: roslaunch franka_control franka_control.launch robot_ip:=192.168.0.88 load_gripper:=true \u2022 Terminal 3: roslaunch panda_moveit_config panda_moveit. launchcontroller:=position \u2022 Terminal 4: roslaunch panda_moveit_config moveit_rviz.launch \u2022 Terminal 5: python \u2018NAMEOFCODE\u2019.py NOTE: to change the \u2018NAMEOFCODE\u2019 as the name of python file to be executed. Image Capture To ensure the light painting will be visible, the surrounding lights should be turned off to increase contrast, while using long exposure photography to capture each frame. The ISO was set to 100 to reduce the graininess of images and a small aperture was used to reduce light entering the camera thus, increasing contrast as well as keeping the entire image in focus (by using a deep depth of field). The camera was mounted on a tripod and an external shutter used to reduce shake during images. The camera was set on \u201cTime\u201d mode, this meant that one press of the shutter would open the aperture and another would close the aperture. Since the duration of motion of the robot arm varied with a frame, this was a simple error-proof method. Before taking photos, the camera was focused on the origin of the end-effector using autofocus and then put on manual focus for the rest of the images. Franka Emika would signal to the human operator to press the shutter by wiggling, at the end of the motion for a frame, the end effector would once again wiggle instructing the end of the previous image and the start of a new frame.","title":"Projectdevelopment"},{"location":"projectdevelopment/#project-development","text":"This section of the documentation consists of the development from animation creation to image capturing","title":"Project Development"},{"location":"projectdevelopment/#method-overview","text":"This project was implemented and segmented into 6 main steps","title":"Method Overview"},{"location":"projectdevelopment/#1-animation-sequencing","text":"The model sequence of the butterfly in flight was generated using Rhinoceros3D and the Grasshopper plugin, iteratvely altering a low-poly mesh to give the effect of a butterfly flapping its wings.","title":"1. Animation Sequencing"},{"location":"projectdevelopment/#2-coordinate-generation","text":"The frame sequence was then treated as individual frames, with the coordinates of each one being extracted and processed to eliminate duplicae points. The coordinates were organised to prevent duplicate paths.","title":"2. Coordinate Generation"},{"location":"projectdevelopment/#3-motion-planning","text":"A motion plan was then generated for each frame using the coordinate list created in the previous step. Code to iterate over the list of frames and plan each motion was developed. This was simulated using RViz and Gazebo.","title":"3. Motion Planning"},{"location":"projectdevelopment/#4-simulation","text":"Once simulated successfully, the code was implemented using Franka Emika\u2019s Panda robotic arm.","title":"4. Simulation"},{"location":"projectdevelopment/#5-implementation","text":"An end-effector attachment containing a controllable LED was simultaneously developed and manufactured to facilitate light painting","title":"5. Implementation"},{"location":"projectdevelopment/#6-image-capture","text":"Once the code successfully ran repeatedly on the Panda robot, long-exposure photgraphs were captured of the motion. These photographs were then collated and displayed in sequence to create an animation of a butterfly in flight.","title":"6. Image Capture"},{"location":"projectdevelopment/#animation-creation","text":"The butterfly model was simplified to a low-poly model since it was expected that a large number of frames would be needed to form a smooth animation. I was also intended that the focus was on the animation itself and not on the model. As the project was limited in terms of time with the robot, a simplified outline presented the most opportunity to iterate and increase robotic complexity. I.e. A low fidelity model reduces the amount of time necessary for the robot to complete each frame. Using slow-motion footage of butterflies in flight, one model for each frame of the animation was created in Rhinoceros 3D . The animation was simplified to 11 frames. The angles of the butterfly wings about the x-axis and the z-axis are approximately sinusoidal. A subtle body wiggle was also added, which is a small, random rotation of the two body triangles about the y-axis. (Angles in reference to Figure B axes). NOTE: Note: Grasshopper is a plugin included in Rhino 6, which has built-in functions called \u201ccomponents\u201d to allow for easy data manipulation. These are the steps which can be repeated to output the same result for each frame in Grasshopper: Each of the 4 triangles which form a butterfly were deconstructed into their vertices, faces, colours and normals using the \u201cdeconstruct mesh\u201d components. This outputs as a list of lists; a list of 4 triangles and each triangle having 3 vertices, where each vertex is an xyz coordinate. The repeated centre vertices were removed by putting the lists through the \u201ccreate set\u201d component. Each sublist was extracted using a \u201clist item\u201d components. Reorder the list in such a way that the robot can move from coordinate to coordinate without retracing the same path twice. NOTE: This is important in order to avoid any brighter spots in the animation frames which would be seen as an anomaly. The result is a list of coordinates in curly brackets. Simple data manipulation was then carried out in a text editor to put this in the required python form which uses square brackets (replace function in Wordpad, replacing \u201c{\u201c and \u201c}\u201d for \u201c[\u201c and \u201c]\u201d respectively. This method allows for the flexibility of changing the butterfly animation frame by frame without the need to manually change each coordinate. This would also allow for more complexity to be added easily on the butterfly models (ie. higher poly models), at the cost of a longer per-frame drawing time. The Grasshopper definition shown in Figure C is robust enough to deal with having curved lines, and would output these as a higher number of interpolated points to form the curves. Given more time with the robot, this change could be implemented.","title":"Animation Creation"},{"location":"projectdevelopment/#motion-planning","text":"Harvey","title":"Motion Planning"},{"location":"projectdevelopment/#simulation","text":"","title":"Simulation"},{"location":"projectdevelopment/#motion-planning-simulation","text":"Before real-life implementation of the code on the Panda robot, the entire task was tested using RViz and Gazebo simulations. This allowed for all poses and time functions to be tested without the risk of damaging the actual robot. It also made it easier to collaborate on and iteratively improve the code as it was not necessary for each team member to individually access the physical robot. The Panda model was loaded, spawned and placed into the Gazebo environment, then controlled with the Python code written by the group members.","title":"Motion Planning Simulation"},{"location":"projectdevelopment/#light-painting-simulation","text":"By tracing the position of the end effector in RViz, it allowed the path of the attached light-painting module to be recreated in the RViz viewport. This method proved to be much more efficient than camera implementation within Gazebo, which required additional plugins (such as a camera sensor plugin to publish the live image to an image topic and LinkPlot3DPlugin to achieve isolated link tracing) and produced a noisier output due to the shaking exhibited within Gazebo simulations. This effectively generated a digital model of the light painting that the Panda robot would recreate in real life. To initialize the light tracing simulation in RViz, go to: Motion Planning Planned Path Links Check Panda_Hand[_] Check Show Trail [_]","title":"Light Painting Simulation"},{"location":"projectdevelopment/#light-source","text":"A custom light torch was designed and manufactured to comply towards the dimensions of the Franka Emika end effector gripper. The STL file of the light stick can be found here and be FDM printed on 0.2mm layer height at 30% infill. The light unit was designed as a hexagonal shape to be held securely on the end effector with a cone tip to direct the light source towards the centre of the unit. A second iteration was made to allow for the unit to be picked up automatically along with a single pole single throw (SPST) switch attached for the light source to stay on when the end effector gripper is closed.","title":"Light Source"},{"location":"projectdevelopment/#attachment","text":"To add in the motion of the light stick being picked up, a magnetic strip was designed to be permanently attached on one end of the end effector. Magnets of the opposite polarity were attached on the main light unite to ensure it remains attached on the end effector when the gripper is opened.","title":"Attachment"},{"location":"projectdevelopment/#circuitry","text":"The internal circuitry of the light unit consists of two 1.5V AA Batteries, connected to a SPST switch and an LED. This would ensure that when the end effector gripper is closed, the light is turned on and vice versa when the gripper is open. This would also remove unwanted bright spots and allow for the possibility of continuous images within the same frame.","title":"Circuitry"},{"location":"projectdevelopment/#implementation-on-panda","text":"","title":"Implementation on Panda"},{"location":"projectdevelopment/#setting-up-franka-emika","text":"First, release the brakes by making sure the external activation device (EAD) is pressed down. Then, in the controller web interface, click to open brakes. A clicking sound will be heard indicating the EAD button needs to be released. After releasing the EAD button, the LED light will turn white, ready to be used.","title":"Setting Up Franka Emika"},{"location":"projectdevelopment/#controlling-franka-ros","text":"With libfranka and franka_ros installed in the real-time Linux kernel, the following steps were carried out before launching the pre-programmed code. Open terminal and type cd franka_ws Then, type source /devel/setup.bash Split the terminal into 5 terminals. In the following order, type in these commands in the respective terminals. \u2022 Terminal 1: roscore \u2022 Terminal 2: roslaunch franka_control franka_control.launch robot_ip:=192.168.0.88 load_gripper:=true \u2022 Terminal 3: roslaunch panda_moveit_config panda_moveit. launchcontroller:=position \u2022 Terminal 4: roslaunch panda_moveit_config moveit_rviz.launch \u2022 Terminal 5: python \u2018NAMEOFCODE\u2019.py NOTE: to change the \u2018NAMEOFCODE\u2019 as the name of python file to be executed.","title":"Controlling Franka &amp; ROS"},{"location":"projectdevelopment/#image-capture","text":"To ensure the light painting will be visible, the surrounding lights should be turned off to increase contrast, while using long exposure photography to capture each frame. The ISO was set to 100 to reduce the graininess of images and a small aperture was used to reduce light entering the camera thus, increasing contrast as well as keeping the entire image in focus (by using a deep depth of field). The camera was mounted on a tripod and an external shutter used to reduce shake during images. The camera was set on \u201cTime\u201d mode, this meant that one press of the shutter would open the aperture and another would close the aperture. Since the duration of motion of the robot arm varied with a frame, this was a simple error-proof method. Before taking photos, the camera was focused on the origin of the end-effector using autofocus and then put on manual focus for the rest of the images. Franka Emika would signal to the human operator to press the shutter by wiggling, at the end of the motion for a frame, the end effector would once again wiggle instructing the end of the previous image and the start of a new frame.","title":"Image Capture"},{"location":"real/","text":"Code Execution On Real Panda Robot This section explains how to run the python code on a real Franka Emika Panda Robot. Before attempting, please follow the setup instructions in the getting started section. Setting Up Panda Robot First, release the brakes by making sure the external activation device (EAD) is pressed down. Then, in the controller web interface, click to open brakes. A clicking sound will be heard indicating the EAD button needs to be released. After releasing the EAD button, the LED light will turn white, ready to be used. Note: Remember to always have a designated person holding the safety stop top prevent damage to the robot during an error Controlling Franka & ROS With libfranka and franka_ros installed in the real-time Linux kernel, the following steps should be carried to set up the robot. Open terminal and type cd franka_ws Then, type source /devel/setup.bash Split the terminal into 5 terminals. In the following order, type in these commands in the respective terminals. In terminal 1 type $roscore In terminal 2 type $roslaunch franka_control franka_control.launch robot_ip:=192.168.0.88 load_gripper:=true In terminal 3 type $roslaunch panda_moveit_config panda_moveit.launchcontroller:=position In terminal 4 type $roslaunch panda_moveit_config moveit_rviz.launch Executing pandaLightPaint The system is now ready to run the pandaLightPaint code In terminal 5 navigate to the folder where you sotred the python code and type $python pandaLightPaint.py The robot should begin drawing the frames in the same way as in the simulation. Please refer to the other guide sections for further information on physical set up of the robot.","title":"Real"},{"location":"real/#code-execution-on-real-panda-robot","text":"This section explains how to run the python code on a real Franka Emika Panda Robot. Before attempting, please follow the setup instructions in the getting started section.","title":"Code Execution On Real Panda Robot"},{"location":"real/#setting-up-panda-robot","text":"First, release the brakes by making sure the external activation device (EAD) is pressed down. Then, in the controller web interface, click to open brakes. A clicking sound will be heard indicating the EAD button needs to be released. After releasing the EAD button, the LED light will turn white, ready to be used. Note: Remember to always have a designated person holding the safety stop top prevent damage to the robot during an error","title":"Setting Up Panda Robot"},{"location":"real/#controlling-franka-ros","text":"With libfranka and franka_ros installed in the real-time Linux kernel, the following steps should be carried to set up the robot. Open terminal and type cd franka_ws Then, type source /devel/setup.bash Split the terminal into 5 terminals. In the following order, type in these commands in the respective terminals. In terminal 1 type $roscore In terminal 2 type $roslaunch franka_control franka_control.launch robot_ip:=192.168.0.88 load_gripper:=true In terminal 3 type $roslaunch panda_moveit_config panda_moveit.launchcontroller:=position In terminal 4 type $roslaunch panda_moveit_config moveit_rviz.launch","title":"Controlling Franka &amp; ROS"},{"location":"real/#executing-pandalightpaint","text":"The system is now ready to run the pandaLightPaint code In terminal 5 navigate to the folder where you sotred the python code and type $python pandaLightPaint.py The robot should begin drawing the frames in the same way as in the simulation. Please refer to the other guide sections for further information on physical set up of the robot.","title":"Executing pandaLightPaint"},{"location":"resource/","text":"","title":"Resources"},{"location":"simulation/","text":"Simulation Setting up RViz and Gazebo for Panda Simulation Save the python file to be simulated in the VM folder catkin_ws/src/franka_gazebo/scripts Start roscore Open a new terminal and type $ roscore Start Gazebo Open a new terminal and type $ cd catkin_ws $ source level/setup.bash $ rosin gazebo_ros gazebo Add Panda Open a new terminal and type $ cd catkin_ws $ source level/setup.bash $ roslaunch franka_gazebo panda_arm_hand.launch You should now see Panda in the Gazebo emulation pointing upwards. Start MoveIt! Open a new terminal and type $ cd catkin_ws $ source devel/setup.bash $ roslaunch panda_moveit_config demo.launch rviz_tutorial:=true RViz will start Add motion planning Make sure the Planning Scene Topic is set to /planning_scene In Planning Request, change the Planning Group to panda_arm You should see Panda in RViz in the same pose as Gazebo Start the node connecting MoveIt! to Gazebo Open a new terminal $ cd catkin_ws $ cd src/panda_publisher $ python panda_publisher.py Updating the Gazebo simulation gains Open a new terminal $ cd catkin_ws $ cd src/franka_gazebo/config $ edit default.yaml Edit the file to match this [image of the yams file] Save and close the file Controlling Panda Open a new terminal $ cd catkin_ws $ cd src/franka_gazebo/scripts $ python \u2018YOUR FILE NAME\u2019.py Motion Planning Simulation Before real-life implementation of the code on the Panda robot, the entire task was tested using RViz and Gazebo simulations. This allowed for all poses and time functions to be tested without the risk of damaging the actual robot. It also made it easier to collaborate on and iteratively improve the code as it was not necessary for each team member to individually access the physical robot. The Panda model was loaded, spawned and placed into the Gazebo environment, then controlled with the Python code written by the group members. Light Painting Simulation By tracing the position of the end effector in RViz, it allowed the path of the attached light-painting module to be recreated in the RViz viewport. This method proved to be much more efficient than camera implementation within Gazebo, which required additional plugins (such as a camera sensor plugin to publish the live image to an image topic and LinkPlot3DPlugin to achieve isolated link tracing) and produced a noisier output due to the shaking exhibited within Gazebo simulations. This effectively generated a digital model of the light painting that the Panda robot would recreate in real life. To initialize the light tracing simulation in RViz, go to: Motion Planning Planned Path Links Check Panda_Hand[_] Check Show Trail [_]","title":"Simulation"},{"location":"simulation/#simulation","text":"","title":"Simulation"},{"location":"simulation/#setting-up-rviz-and-gazebo-for-panda-simulation","text":"Save the python file to be simulated in the VM folder catkin_ws/src/franka_gazebo/scripts Start roscore Open a new terminal and type $ roscore Start Gazebo Open a new terminal and type $ cd catkin_ws $ source level/setup.bash $ rosin gazebo_ros gazebo Add Panda Open a new terminal and type $ cd catkin_ws $ source level/setup.bash $ roslaunch franka_gazebo panda_arm_hand.launch You should now see Panda in the Gazebo emulation pointing upwards. Start MoveIt! Open a new terminal and type $ cd catkin_ws $ source devel/setup.bash $ roslaunch panda_moveit_config demo.launch rviz_tutorial:=true RViz will start Add motion planning Make sure the Planning Scene Topic is set to /planning_scene In Planning Request, change the Planning Group to panda_arm You should see Panda in RViz in the same pose as Gazebo Start the node connecting MoveIt! to Gazebo Open a new terminal $ cd catkin_ws $ cd src/panda_publisher $ python panda_publisher.py Updating the Gazebo simulation gains Open a new terminal $ cd catkin_ws $ cd src/franka_gazebo/config $ edit default.yaml Edit the file to match this [image of the yams file] Save and close the file Controlling Panda Open a new terminal $ cd catkin_ws $ cd src/franka_gazebo/scripts $ python \u2018YOUR FILE NAME\u2019.py","title":"Setting up RViz and Gazebo for Panda Simulation"},{"location":"simulation/#motion-planning-simulation","text":"Before real-life implementation of the code on the Panda robot, the entire task was tested using RViz and Gazebo simulations. This allowed for all poses and time functions to be tested without the risk of damaging the actual robot. It also made it easier to collaborate on and iteratively improve the code as it was not necessary for each team member to individually access the physical robot. The Panda model was loaded, spawned and placed into the Gazebo environment, then controlled with the Python code written by the group members.","title":"Motion Planning Simulation"},{"location":"simulation/#light-painting-simulation","text":"By tracing the position of the end effector in RViz, it allowed the path of the attached light-painting module to be recreated in the RViz viewport. This method proved to be much more efficient than camera implementation within Gazebo, which required additional plugins (such as a camera sensor plugin to publish the live image to an image topic and LinkPlot3DPlugin to achieve isolated link tracing) and produced a noisier output due to the shaking exhibited within Gazebo simulations. This effectively generated a digital model of the light painting that the Panda robot would recreate in real life. To initialize the light tracing simulation in RViz, go to: Motion Planning Planned Path Links Check Panda_Hand[_] Check Show Trail [_]","title":"Light Painting Simulation"},{"location":"software/","text":"Setting Up Software The following are the instructions on setting up a virtual machine for Windows or Mac Installing VMware Workstation 15 (Windows) Go to the downloads page on the VMware website Sign up for a VMware account or log in if you already have an account Accept the End User License Agreement Download the installer file; VMware-workstation-full-15.5.1-15018442.exe Run the installer Installing VMware Fusion (Mac) Go to the downloads page on the VMware website Sign up for a VMware account or log in if you already have an account Accept the End User License Agreement Download the installer file; VMware-Fusion-11.5.1-15018442.dmg Run the installer Installing ROS Virtual Machine Download the Virtual machine archive file (.zip) from here . Extract the zip archive. It contains a single virtual machine (one .vmx file and one or more .vmdk files). Run VMware Workstation (or VMware Fusion on Mac), click \u201copen an existing VM\u201d and select the .vmx file. Start the VM by clicking on the \u201cplay\u201d button. Inside the VM, log in with the credentials: login: user password learn Once you have both the VMWare and ROS Virtual Machine installed and running, you are ready to begin running simulations.","title":"Software"},{"location":"software/#setting-up-software","text":"The following are the instructions on setting up a virtual machine for Windows or Mac","title":"Setting Up Software"},{"location":"software/#installing-vmware-workstation-15-windows","text":"Go to the downloads page on the VMware website Sign up for a VMware account or log in if you already have an account Accept the End User License Agreement Download the installer file; VMware-workstation-full-15.5.1-15018442.exe Run the installer","title":"Installing VMware Workstation 15 (Windows)"},{"location":"software/#installing-vmware-fusion-mac","text":"Go to the downloads page on the VMware website Sign up for a VMware account or log in if you already have an account Accept the End User License Agreement Download the installer file; VMware-Fusion-11.5.1-15018442.dmg Run the installer","title":"Installing VMware Fusion (Mac)"},{"location":"software/#installing-ros-virtual-machine","text":"Download the Virtual machine archive file (.zip) from here . Extract the zip archive. It contains a single virtual machine (one .vmx file and one or more .vmdk files). Run VMware Workstation (or VMware Fusion on Mac), click \u201copen an existing VM\u201d and select the .vmx file. Start the VM by clicking on the \u201cplay\u201d button. Inside the VM, log in with the credentials: login: user password learn Once you have both the VMWare and ROS Virtual Machine installed and running, you are ready to begin running simulations.","title":"Installing ROS Virtual Machine"},{"location":"trouble/","text":"","title":"Troubleshooting"},{"location":"vid/","text":"Reference Videos A number of reference videos have been created to help you get started with the project. Overview Video - Short video explaining overall concept of project with exeution of butterfly example and output animation Overview Video Code Execution Tutorial - Detailed tutorial on how to run the code in the Gazebo simulator. Use this in addition to the documentation in project guide. Code Execution Tutorial Real Robot Execution - Short video documenting the Panda robot executing the code in real life Real Robot Execution Simulator Execution - Short video documenting the code being executing in the simulator with virtual lightpainting in RViz Simulator Execution","title":"Videos"},{"location":"vid/#reference-videos","text":"A number of reference videos have been created to help you get started with the project. Overview Video - Short video explaining overall concept of project with exeution of butterfly example and output animation Overview Video Code Execution Tutorial - Detailed tutorial on how to run the code in the Gazebo simulator. Use this in addition to the documentation in project guide. Code Execution Tutorial Real Robot Execution - Short video documenting the Panda robot executing the code in real life Real Robot Execution Simulator Execution - Short video documenting the code being executing in the simulator with virtual lightpainting in RViz Simulator Execution","title":"Reference Videos"}]}