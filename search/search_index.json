{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About This is the documentation of The Butterfly Director (TBD) group, for the Imperial College London, Design Engineering Third Year (DE3), Robotics module (March 2020). This project is hosted on GitHub Team Harvey Upton Tom\u00e1\u0161 K\u0148aze David Prior Hope Pauline Ng Luke Hillery Higor Alves De Freitas For enquiries on this documentation, please contact meng.ng17@imperial.ac.uk For help with our code, please contact either harvey.upton17@imperial.ac.uk or tomas.knaze17@imperial.ac.uk Summary Drawing inspiration from light painting images, this project entails programming the Franka Emika robotic arm to draw several still images of a butterfly with light to produce a stop motion animation. The Franka Emika Panda was used as it can move very precisely making it suitable for recreating small details in 3D images. Method Overview This project was implemented and segmented into 6 main steps 1. Animation Sequencing The model sequence of the butterfly in flight was generated using Rhinoceros3D and the Grasshopper plugin, iteratvely altering a low-poly mesh to give the effect of a butterfly flapping its wings. 2. Coordinate Generation The frame sequence was then treated as individual frames, with the coordinates of each one being extracted and processed to eliminate duplicae points. The coordinates were organised to prevent duplicate paths. 3. Motion Planning A motion plan was then generated for each frame using the coordinate list created in the previous step. Code to iterate over the list of frames and plan each motion was developed. This was simulated using RViz and Gazebo. 4. Simulation Once simulated successfully, the code was implemented using Franka Emika\u2019s Panda robotic arm. 5. Implementation An end-effector attachment containing a controllable LED was simultaneously developed and manufactured to facilitate light painting 6. Image Capture Once the code successfully ran repeatedly on the Panda robot, long-exposure photgraphs were captured of the motion. These photographs were then collated and displayed in sequence to create an animation of a butterfly in flight.","title":"Introduction"},{"location":"#about","text":"This is the documentation of The Butterfly Director (TBD) group, for the Imperial College London, Design Engineering Third Year (DE3), Robotics module (March 2020). This project is hosted on GitHub","title":"About"},{"location":"#team","text":"Harvey Upton Tom\u00e1\u0161 K\u0148aze David Prior Hope Pauline Ng Luke Hillery Higor Alves De Freitas For enquiries on this documentation, please contact meng.ng17@imperial.ac.uk For help with our code, please contact either harvey.upton17@imperial.ac.uk or tomas.knaze17@imperial.ac.uk","title":"Team"},{"location":"#summary","text":"Drawing inspiration from light painting images, this project entails programming the Franka Emika robotic arm to draw several still images of a butterfly with light to produce a stop motion animation. The Franka Emika Panda was used as it can move very precisely making it suitable for recreating small details in 3D images.","title":"Summary"},{"location":"#method-overview","text":"This project was implemented and segmented into 6 main steps","title":"Method Overview"},{"location":"#1-animation-sequencing","text":"The model sequence of the butterfly in flight was generated using Rhinoceros3D and the Grasshopper plugin, iteratvely altering a low-poly mesh to give the effect of a butterfly flapping its wings.","title":"1. Animation Sequencing"},{"location":"#2-coordinate-generation","text":"The frame sequence was then treated as individual frames, with the coordinates of each one being extracted and processed to eliminate duplicae points. The coordinates were organised to prevent duplicate paths.","title":"2. Coordinate Generation"},{"location":"#3-motion-planning","text":"A motion plan was then generated for each frame using the coordinate list created in the previous step. Code to iterate over the list of frames and plan each motion was developed. This was simulated using RViz and Gazebo.","title":"3. Motion Planning"},{"location":"#4-simulation","text":"Once simulated successfully, the code was implemented using Franka Emika\u2019s Panda robotic arm.","title":"4. Simulation"},{"location":"#5-implementation","text":"An end-effector attachment containing a controllable LED was simultaneously developed and manufactured to facilitate light painting","title":"5. Implementation"},{"location":"#6-image-capture","text":"Once the code successfully ran repeatedly on the Panda robot, long-exposure photgraphs were captured of the motion. These photographs were then collated and displayed in sequence to create an animation of a butterfly in flight.","title":"6. Image Capture"},{"location":"CodeDevelopement/","text":"Code Development This section talks about how the final python code works and why it is set up in this way. The final code is available on GitHub at this link https://github.com/HarveyU1/LightPaintingRobot. Code Structure There are two main python files which are used to get the panda robot to light paint. -pandaLightPaint is the main python file which is executed from ROS to carry out motion planning for each frame of the animation and then executes this motion plan by moving the end effector and controlling the gripper position. -frames is the python file in which the cartesian based trajectory is stored for a complete animation. This is imported by pandaLightPaint and informs the motion plan which is calculated. Main Code (pandaLightPaint) The main python code that is launched from the terminal. Set Up First, the required python modules are imported, ensure that all of these are installed in your python installation within ROS. Note ROS specific modules such as different message formats which allow communication between nodes. #!/usr/bin/env python #ROS specific import sys import copy import rospy import moveit_commander import moveit_msgs.msg import geometry_msgs.msg from std_msgs.msg import String, Float64MultiArray, MultiArrayDimension, Float64 from moveit_commander.conversions import pose_to_list Next, the generic python packages are imported. math.pi is required for robot angle calculations which are based in radians from math import pi import tf Finally, the list framelist is imported from the frames python file which is stored alongside pandaLightPaint from frames import framelist Modifiable variables These are the variables which can be changed to modify the behaviour of the robot. Some of these are useful to modify when troubleshooting for problems as they can often help prevent errors. The offset coordinates are the cartesian coordinates in metres of the starting position for all frame drawing. The frame coordinates, in framelist, are provided w.r.t to a central local origin point. In the case of the butterfly, this is the centre of its body. The offset coordinated oOffCords specify the location of the frames local coordinate system relative to the robots coordinate system which has an origin located at the base of the arm. By default, the offset coordinates are set to 0.5 metres in front of the robot(x-axis) and 0.5 metres height above the base of the robot(z-axis). This always for a good amount of redundancy when drawing frames as the arm is well within the dexterous workspace. oOffCords = [0.5,0,0.5] The offset rotation specifies the desired pitch, roll and yaw of the end effector during light painting. When motion planning this is added as a constraint to ensure the end effector is always facing in the same direction. This means that the light will always be facing towards the camera and prevents distortion of the frame draw. By default, this is set to a pitch of pi radians and yaw of -pi/4 radians which correspond to custom torch attachment facing directly forward. These values can be changed if the camera is positioned differently. oOffRots = [pi,0,-pi/4] The shake angle simply specified the angle in radians by which the end effector will be offset in yaw during the shake manoeuvre. Generally, this will not need to be changed unless you want the shake to be fast in which case the angle should be reduced. shakeAngle = pi/8 Frame Scale is used to adjust the dimensions of the frames within framelist for the animation. Depending on the unit of scale for the animation the frame coordinates need to be scaled to fit with the dexterous workspace of the robot. If the coordinate is outside the workspace then there will either be a motion planning error or the robot will fail to execute the motion plan. In this, the framescale should be reduced to ensure all coordinates are within the acceptable range. Generally, the framescale can be determined by finding the value of the largest coordinate then choosing a framescale which multiplies this value to equal 0.3. This ensures that the frame will all be drawn within a 0.6-metre sized box with its centre and the position of the offset coordinate. framescale = 0.3 / Largest Coordinate This value is not automatically calculated as users may want to experiment with drawing larger frames or drawing smaller frames to save time. Additionally, if multiple frames files are used it may be required that these are all scaled by the same value. frameScale = 0.004 Processing time is simply the length of time in seconds between the drawing of each frame. This allows time for the camera to process the long exposure photo. This value can be changed to suit the processing time for whichever camera is being used. processingTime = 20 Light on and light off distances are the distances in metres which the end effector grabber will open and close to when turning the light on and off during frame drawing. The default values work well for the custom end effector light specified in this documentation. If a different torch design is used these values can be changed as required. lightOnDist = 0.03 lightOffDist = 0.05 Main Execution Set Up This is the main section of the code which is executed when the program is launched from the terminal. It includes the initial set up as the main loop which is responsible for drawing each frame. To begin with rospy a new node is created for the panda light controller. Next, the motion planing is initialised with the group name being assigned. rospy.init_node('panda_light_paint_controller', anonymous=True) #Initialise motion planning in RVIZ moveit_commander.roscpp_initialize(sys.argv) robot = moveit_commander.RobotCommander() scene = moveit_commander.PlanningSceneInterface() group_name = \"panda_arm\" group = moveit_commander.MoveGroupCommander(group_name) A publisher is created for the controlling the gripper. The message format for the gripper is also specified as a 2d array of float values. gripper_publisher = rospy.Publisher('/franka/gripper_position_controller/command', Float64MultiArray, queue_size=1) gripper_msg = Float64MultiArray() # gripper_msg.layout.dim = [MultiArrayDimension('', 2, 1)] Finally the goToPose function is called to set up the robot in it's starting configuration with the end effector at the offset positition at the offset rotation values. goToPose(oOffRots[0],oOffRots[1],oOffRots[2],oOffCords[0],oOffCords[1],oOffCords[2]) Main Execution Loop The main execution loop will be carried out for each frame of the animation in framelist. Each time this loop is executed a frame will be drawn. The loop starts with a terminal message informing the user to open the shutter on the camera. Next, the lightOn function is called which closes the grippers to turn on the torch held by panda. The goOnPath function is executed which draws one frame of the animation. The light is then turned off and a terminal message is printed informing the user to end the current photo. Finally, a delay is carried out to allow time for the camera to process the image before the next frame. for frame in framelist: #User inform message to start photo print(\"Open Shutter\") #Turn on the light by closing the end effector lightOn() #Wait 1 second rospy.sleep(1) #Execute frame motion goOnPath(frame) #Turn off the light by opening the end effector lightOff() #User inform message to end photo print(\"Close Shutter\") #Wait time so that camera has enough time to process image before next frame rospy.sleep(processingTime) goToPose Function This function takes the input arguments roll, pitch and yaw, as well as x,y and z coordinates, and will calculate and execute a motion plan to get the robot in the correct pose. This is done by first clearing the old pose targets. Then converting the rotation values into a quaternion which is used to set the new values for the origin pose (which is the target pose). The motion plan is then calculated and executed. If a motion plan cant be found for the chosen pose values and an error message will be displayed in the terminal. def goToPose(roll,pitch,yaw,xCord,yCord,zCord): group.clear_pose_targets() quaternion = tf.transformations.quaternion_from_euler(roll, pitch, yaw) origin_pose = geometry_msgs.msg.Pose() origin_pose.orientation.x = quaternion[0] origin_pose.orientation.y = quaternion[1] origin_pose.orientation.z = quaternion[2] origin_pose.orientation.w = quaternion[3] origin_pose.position.x = xCord origin_pose.position.y = yCord origin_pose.position.z = zCord group.set_pose_target(origin_pose) plan = group.go(wait=True) shakeHand Function This function will execute a number of handshakes specified by the quantity input argument. These handshakes can be used to indicate the start and end of frames to the user when operating the robot if the terminal is not visible by the camera operator. This function can deal with invalid input arguments and will only execute if the integer of the input quantity is greater than 0. An error message will be displayed for invalid input. def shakeHand(quantity): quantity = int(quantity) #Check if the quanitty value is valid, if not print error message if (quantity >=1): for i in range (quantity): goToPose(oOffRots[0],oOffRots[1],oOffRots[2]-shakeAngle,oOffCords[0],oOffCords[1],oOffCords[2]) goToPose(oOffRots[0],oOffRots[1],oOffRots[2]+shakeAngle,oOffCords[0],oOffCords[1],oOffCords[2]) goToPose(oOffRots[0],oOffRots[1],oOffRots[2],oOffCords[0],oOffCords[1],oOffCords[2]) else: print(\"Shake not executed. Shake quantity value needs to be a positive integer\") goOnPath Function This function takes a frame of coordinates, as it's input argument, and then executes the drawing of the frame by calculating a motion plan and executing it on the robot. This function works similarly to the goToPose function, however, group.compute_cartesian_path is used to calculate the motion plan as the trajectory is based upon multiple coordinates. The coordinates that make up a frame are converted into drawpose values inside the for loop and these are then added to the list of poses called drawpoints. The motion plan is calculated based on this list of robot poses. def goOnPath(frame): #Clear previous pose targets group.clear_pose_targets() #Create an empty list to store the frame coordinates drawpoints = [] for coordinate in frame: #Calculate the quaternion of the end effector facing forward quaternion = tf.transformations.quaternion_from_euler(oOffRots[0],oOffRots[1],oOffRots[2]) #Initialise the drawpose message drawpose = geometry_msgs.msg.Pose() #update the drawpose message base upon the required rotation and coordinate location of the end effector drawpose.orientation.x = quaternion[0] drawpose.orientation.y = quaternion[1] drawpose.orientation.z = quaternion[2] drawpose.orientation.w = quaternion[3] drawpose.position.x = oOffCords[0] + coordinate[0] *frameScale drawpose.position.y = oOffCords[1] + coordinate[1] *frameScale drawpose.position.z = oOffCords[2] + coordinate[2] *frameScale #Add the drawpose message to the list of drawpoints drawpoints.append(copy.deepcopy(drawpose)) #Calculate motion path base on drawpoints list (plan, fraction) = group.compute_cartesian_path(drawpoints, 0.01, 0.0) #Execute the calculated motion plan and wait until complete group.execute(plan, wait=True) lightOn Function This function will turn on the light attached to the end effector by closing the gripper based upon the value set in the lightOnDist variable. def lightOn(): gripper_msg.data = [lightOnDist, lightOnDist] gripper_publisher.publish(gripper_msg) lightOff Function This function will turn off the light attached to the end effector by opening the gripper based upon the value set in the lightOffDist variable. def lightOff(): gripper_msg.data = [lightOffDist, lightOffDist] gripper_publisher.publish(gripper_msg) Storing Animation (frames) The animation frames are stored in the framelist in the python file frames. This is the output data from the animation creation process in Rhino. Each frame is comprised of a list of 3 value lists which contain a coordinate value [x,y,z]. Multiple frames make up an animation sequence. An example of one frame from the butterfly animation can be seen below. framelist = [[[0, 0, 0], [-30.112995, 0, 4.267024], [-29.869165, 0, -5.730003], [0, 0, 0], [19.752333, 0, 5.902996], [20.161001, 0, -3.087721], [0, 0, 0], [-26.639191, -9.011853, -26.175182], [26.034624, -20.905941, -26.17518], [0, 0, 0], [-26.639191, 9.011853, -26.175182], [26.034624, 20.905941, -26.17518], [0, 0, 0.1]]]","title":"Code Development"},{"location":"CodeDevelopement/#code-development","text":"This section talks about how the final python code works and why it is set up in this way. The final code is available on GitHub at this link https://github.com/HarveyU1/LightPaintingRobot.","title":"Code Development"},{"location":"CodeDevelopement/#code-structure","text":"There are two main python files which are used to get the panda robot to light paint. -pandaLightPaint is the main python file which is executed from ROS to carry out motion planning for each frame of the animation and then executes this motion plan by moving the end effector and controlling the gripper position. -frames is the python file in which the cartesian based trajectory is stored for a complete animation. This is imported by pandaLightPaint and informs the motion plan which is calculated.","title":"Code Structure"},{"location":"CodeDevelopement/#main-code-pandalightpaint","text":"The main python code that is launched from the terminal.","title":"Main Code (pandaLightPaint)"},{"location":"CodeDevelopement/#set-up","text":"First, the required python modules are imported, ensure that all of these are installed in your python installation within ROS. Note ROS specific modules such as different message formats which allow communication between nodes. #!/usr/bin/env python #ROS specific import sys import copy import rospy import moveit_commander import moveit_msgs.msg import geometry_msgs.msg from std_msgs.msg import String, Float64MultiArray, MultiArrayDimension, Float64 from moveit_commander.conversions import pose_to_list Next, the generic python packages are imported. math.pi is required for robot angle calculations which are based in radians from math import pi import tf Finally, the list framelist is imported from the frames python file which is stored alongside pandaLightPaint from frames import framelist","title":"Set Up"},{"location":"CodeDevelopement/#modifiable-variables","text":"These are the variables which can be changed to modify the behaviour of the robot. Some of these are useful to modify when troubleshooting for problems as they can often help prevent errors. The offset coordinates are the cartesian coordinates in metres of the starting position for all frame drawing. The frame coordinates, in framelist, are provided w.r.t to a central local origin point. In the case of the butterfly, this is the centre of its body. The offset coordinated oOffCords specify the location of the frames local coordinate system relative to the robots coordinate system which has an origin located at the base of the arm. By default, the offset coordinates are set to 0.5 metres in front of the robot(x-axis) and 0.5 metres height above the base of the robot(z-axis). This always for a good amount of redundancy when drawing frames as the arm is well within the dexterous workspace. oOffCords = [0.5,0,0.5] The offset rotation specifies the desired pitch, roll and yaw of the end effector during light painting. When motion planning this is added as a constraint to ensure the end effector is always facing in the same direction. This means that the light will always be facing towards the camera and prevents distortion of the frame draw. By default, this is set to a pitch of pi radians and yaw of -pi/4 radians which correspond to custom torch attachment facing directly forward. These values can be changed if the camera is positioned differently. oOffRots = [pi,0,-pi/4] The shake angle simply specified the angle in radians by which the end effector will be offset in yaw during the shake manoeuvre. Generally, this will not need to be changed unless you want the shake to be fast in which case the angle should be reduced. shakeAngle = pi/8 Frame Scale is used to adjust the dimensions of the frames within framelist for the animation. Depending on the unit of scale for the animation the frame coordinates need to be scaled to fit with the dexterous workspace of the robot. If the coordinate is outside the workspace then there will either be a motion planning error or the robot will fail to execute the motion plan. In this, the framescale should be reduced to ensure all coordinates are within the acceptable range. Generally, the framescale can be determined by finding the value of the largest coordinate then choosing a framescale which multiplies this value to equal 0.3. This ensures that the frame will all be drawn within a 0.6-metre sized box with its centre and the position of the offset coordinate. framescale = 0.3 / Largest Coordinate This value is not automatically calculated as users may want to experiment with drawing larger frames or drawing smaller frames to save time. Additionally, if multiple frames files are used it may be required that these are all scaled by the same value. frameScale = 0.004 Processing time is simply the length of time in seconds between the drawing of each frame. This allows time for the camera to process the long exposure photo. This value can be changed to suit the processing time for whichever camera is being used. processingTime = 20 Light on and light off distances are the distances in metres which the end effector grabber will open and close to when turning the light on and off during frame drawing. The default values work well for the custom end effector light specified in this documentation. If a different torch design is used these values can be changed as required. lightOnDist = 0.03 lightOffDist = 0.05","title":"Modifiable variables"},{"location":"CodeDevelopement/#main-execution-set-up","text":"This is the main section of the code which is executed when the program is launched from the terminal. It includes the initial set up as the main loop which is responsible for drawing each frame. To begin with rospy a new node is created for the panda light controller. Next, the motion planing is initialised with the group name being assigned. rospy.init_node('panda_light_paint_controller', anonymous=True) #Initialise motion planning in RVIZ moveit_commander.roscpp_initialize(sys.argv) robot = moveit_commander.RobotCommander() scene = moveit_commander.PlanningSceneInterface() group_name = \"panda_arm\" group = moveit_commander.MoveGroupCommander(group_name) A publisher is created for the controlling the gripper. The message format for the gripper is also specified as a 2d array of float values. gripper_publisher = rospy.Publisher('/franka/gripper_position_controller/command', Float64MultiArray, queue_size=1) gripper_msg = Float64MultiArray() # gripper_msg.layout.dim = [MultiArrayDimension('', 2, 1)] Finally the goToPose function is called to set up the robot in it's starting configuration with the end effector at the offset positition at the offset rotation values. goToPose(oOffRots[0],oOffRots[1],oOffRots[2],oOffCords[0],oOffCords[1],oOffCords[2])","title":"Main Execution Set Up"},{"location":"CodeDevelopement/#main-execution-loop","text":"The main execution loop will be carried out for each frame of the animation in framelist. Each time this loop is executed a frame will be drawn. The loop starts with a terminal message informing the user to open the shutter on the camera. Next, the lightOn function is called which closes the grippers to turn on the torch held by panda. The goOnPath function is executed which draws one frame of the animation. The light is then turned off and a terminal message is printed informing the user to end the current photo. Finally, a delay is carried out to allow time for the camera to process the image before the next frame. for frame in framelist: #User inform message to start photo print(\"Open Shutter\") #Turn on the light by closing the end effector lightOn() #Wait 1 second rospy.sleep(1) #Execute frame motion goOnPath(frame) #Turn off the light by opening the end effector lightOff() #User inform message to end photo print(\"Close Shutter\") #Wait time so that camera has enough time to process image before next frame rospy.sleep(processingTime)","title":"Main Execution Loop"},{"location":"CodeDevelopement/#gotopose-function","text":"This function takes the input arguments roll, pitch and yaw, as well as x,y and z coordinates, and will calculate and execute a motion plan to get the robot in the correct pose. This is done by first clearing the old pose targets. Then converting the rotation values into a quaternion which is used to set the new values for the origin pose (which is the target pose). The motion plan is then calculated and executed. If a motion plan cant be found for the chosen pose values and an error message will be displayed in the terminal. def goToPose(roll,pitch,yaw,xCord,yCord,zCord): group.clear_pose_targets() quaternion = tf.transformations.quaternion_from_euler(roll, pitch, yaw) origin_pose = geometry_msgs.msg.Pose() origin_pose.orientation.x = quaternion[0] origin_pose.orientation.y = quaternion[1] origin_pose.orientation.z = quaternion[2] origin_pose.orientation.w = quaternion[3] origin_pose.position.x = xCord origin_pose.position.y = yCord origin_pose.position.z = zCord group.set_pose_target(origin_pose) plan = group.go(wait=True)","title":"goToPose Function"},{"location":"CodeDevelopement/#shakehand-function","text":"This function will execute a number of handshakes specified by the quantity input argument. These handshakes can be used to indicate the start and end of frames to the user when operating the robot if the terminal is not visible by the camera operator. This function can deal with invalid input arguments and will only execute if the integer of the input quantity is greater than 0. An error message will be displayed for invalid input. def shakeHand(quantity): quantity = int(quantity) #Check if the quanitty value is valid, if not print error message if (quantity >=1): for i in range (quantity): goToPose(oOffRots[0],oOffRots[1],oOffRots[2]-shakeAngle,oOffCords[0],oOffCords[1],oOffCords[2]) goToPose(oOffRots[0],oOffRots[1],oOffRots[2]+shakeAngle,oOffCords[0],oOffCords[1],oOffCords[2]) goToPose(oOffRots[0],oOffRots[1],oOffRots[2],oOffCords[0],oOffCords[1],oOffCords[2]) else: print(\"Shake not executed. Shake quantity value needs to be a positive integer\")","title":"shakeHand Function"},{"location":"CodeDevelopement/#goonpath-function","text":"This function takes a frame of coordinates, as it's input argument, and then executes the drawing of the frame by calculating a motion plan and executing it on the robot. This function works similarly to the goToPose function, however, group.compute_cartesian_path is used to calculate the motion plan as the trajectory is based upon multiple coordinates. The coordinates that make up a frame are converted into drawpose values inside the for loop and these are then added to the list of poses called drawpoints. The motion plan is calculated based on this list of robot poses. def goOnPath(frame): #Clear previous pose targets group.clear_pose_targets() #Create an empty list to store the frame coordinates drawpoints = [] for coordinate in frame: #Calculate the quaternion of the end effector facing forward quaternion = tf.transformations.quaternion_from_euler(oOffRots[0],oOffRots[1],oOffRots[2]) #Initialise the drawpose message drawpose = geometry_msgs.msg.Pose() #update the drawpose message base upon the required rotation and coordinate location of the end effector drawpose.orientation.x = quaternion[0] drawpose.orientation.y = quaternion[1] drawpose.orientation.z = quaternion[2] drawpose.orientation.w = quaternion[3] drawpose.position.x = oOffCords[0] + coordinate[0] *frameScale drawpose.position.y = oOffCords[1] + coordinate[1] *frameScale drawpose.position.z = oOffCords[2] + coordinate[2] *frameScale #Add the drawpose message to the list of drawpoints drawpoints.append(copy.deepcopy(drawpose)) #Calculate motion path base on drawpoints list (plan, fraction) = group.compute_cartesian_path(drawpoints, 0.01, 0.0) #Execute the calculated motion plan and wait until complete group.execute(plan, wait=True)","title":"goOnPath Function"},{"location":"CodeDevelopement/#lighton-function","text":"This function will turn on the light attached to the end effector by closing the gripper based upon the value set in the lightOnDist variable. def lightOn(): gripper_msg.data = [lightOnDist, lightOnDist] gripper_publisher.publish(gripper_msg)","title":"lightOn Function"},{"location":"CodeDevelopement/#lightoff-function","text":"This function will turn off the light attached to the end effector by opening the gripper based upon the value set in the lightOffDist variable. def lightOff(): gripper_msg.data = [lightOffDist, lightOffDist] gripper_publisher.publish(gripper_msg)","title":"lightOff Function"},{"location":"CodeDevelopement/#storing-animation-frames","text":"The animation frames are stored in the framelist in the python file frames. This is the output data from the animation creation process in Rhino. Each frame is comprised of a list of 3 value lists which contain a coordinate value [x,y,z]. Multiple frames make up an animation sequence. An example of one frame from the butterfly animation can be seen below. framelist = [[[0, 0, 0], [-30.112995, 0, 4.267024], [-29.869165, 0, -5.730003], [0, 0, 0], [19.752333, 0, 5.902996], [20.161001, 0, -3.087721], [0, 0, 0], [-26.639191, -9.011853, -26.175182], [26.034624, -20.905941, -26.17518], [0, 0, 0], [-26.639191, 9.011853, -26.175182], [26.034624, 20.905941, -26.17518], [0, 0, 0.1]]]","title":"Storing Animation (frames)"},{"location":"CodeExecutionInSimulation/","text":"Code Execution in Simulation This section explains how to run the python code on a Panda Robot Simulation. Before attempting to light paint using a real robot it is recommended to first get it working in the Gazebo simulator within your virtual machine. This allows you to test your custom animations and verify everything is working without wasting time on a real robot. 1. Downloading Code Once you have completed the previous steps to set up ROS on your computer, with the required modules, you can now use the light painting code in the Gazebo simulator. Up to date files can be found on GitHub at this link . Download pandaLightPaint.py and frames.py onto your Linux virtual machine. 2. Launching Gazebo Open a new terminal window and divide it into 6 windows and type in the following. In terminal 1 type $roscore In terminal 2 type $cd catkin_ws $cd source devel/setup.bash $rosrun gazebo_ros gazebo In terminal 3 type $cd catkin_ws $source devel/setup.bash $roslaunch franka_gazebo panda_arm_hand.launch You should see that the gazebo simulator has launched and a model of the panda robot can be seen pointing upwards in the environment. 3. Launching MoveIt To launch and set up MoveIt for motion planning do the following In terminal 4 type $cd_catkin_ws $source devel/setup.bash $roslaunch panda_moveit_config demo.launch rviz_tutorial:=true Once MoveIt has started, add motion planning and change the planning scene topic field to /planning_scene In planning request, change planning group to panda_arm 4. Light Trails in MoveIt By tracing the position of the end effector in RViz, it allows the path of the attached light-painting module to be recreated in the RViz viewport which helps when testing new animations. This effectively generates a digital model of the light painting that the Panda robot would recreate in real life. To initialize the light tracing simulation in RViz, go to: 1. Motion Planning 2. Planned Path 3. Links 4. Check Panda_Hand[_] 5. Tick the box Show Trail [_] When the simulation is running in RViz, you will see a line which shows the trajectory the end effector will move along. 5. Connect MoveIt to Gazebo To create the node which connects moveIt with Gazebo, type the following In terminal 5 type $cd catkin_ws $cd src/panda_publisher $python panda_publisher.py 6. launching pandaLightPaint Everything is now set for the simulation so you can launch the pandaLightPaint python code In terminal 6 type $cd catkin_ws cd to the folder you stored the code in $python pandaLightPaint.py The first time you run this you will get some error messages as the robot starts at singularity in the simulation. After about a minute it will find a valid motion plan to get out of singularity. At this point cancel the execution using ctrl c then start the code again. It should now run properly without errors. You should see the robot drawing individual frames in both gazebo and RViz and pausing in between each frame to allow the photo to be processed.","title":"Execution in Simulation"},{"location":"CodeExecutionInSimulation/#code-execution-in-simulation","text":"This section explains how to run the python code on a Panda Robot Simulation. Before attempting to light paint using a real robot it is recommended to first get it working in the Gazebo simulator within your virtual machine. This allows you to test your custom animations and verify everything is working without wasting time on a real robot.","title":"Code Execution in Simulation"},{"location":"CodeExecutionInSimulation/#1-downloading-code","text":"Once you have completed the previous steps to set up ROS on your computer, with the required modules, you can now use the light painting code in the Gazebo simulator. Up to date files can be found on GitHub at this link . Download pandaLightPaint.py and frames.py onto your Linux virtual machine.","title":"1. Downloading Code"},{"location":"CodeExecutionInSimulation/#2-launching-gazebo","text":"Open a new terminal window and divide it into 6 windows and type in the following. In terminal 1 type $roscore In terminal 2 type $cd catkin_ws $cd source devel/setup.bash $rosrun gazebo_ros gazebo In terminal 3 type $cd catkin_ws $source devel/setup.bash $roslaunch franka_gazebo panda_arm_hand.launch You should see that the gazebo simulator has launched and a model of the panda robot can be seen pointing upwards in the environment.","title":"2. Launching Gazebo"},{"location":"CodeExecutionInSimulation/#3-launching-moveit","text":"To launch and set up MoveIt for motion planning do the following In terminal 4 type $cd_catkin_ws $source devel/setup.bash $roslaunch panda_moveit_config demo.launch rviz_tutorial:=true Once MoveIt has started, add motion planning and change the planning scene topic field to /planning_scene In planning request, change planning group to panda_arm","title":"3. Launching MoveIt"},{"location":"CodeExecutionInSimulation/#4-light-trails-in-moveit","text":"By tracing the position of the end effector in RViz, it allows the path of the attached light-painting module to be recreated in the RViz viewport which helps when testing new animations. This effectively generates a digital model of the light painting that the Panda robot would recreate in real life. To initialize the light tracing simulation in RViz, go to: 1. Motion Planning 2. Planned Path 3. Links 4. Check Panda_Hand[_] 5. Tick the box Show Trail [_] When the simulation is running in RViz, you will see a line which shows the trajectory the end effector will move along.","title":"4. Light Trails in MoveIt"},{"location":"CodeExecutionInSimulation/#5-connect-moveit-to-gazebo","text":"To create the node which connects moveIt with Gazebo, type the following In terminal 5 type $cd catkin_ws $cd src/panda_publisher $python panda_publisher.py","title":"5. Connect MoveIt to Gazebo"},{"location":"CodeExecutionInSimulation/#6-launching-pandalightpaint","text":"Everything is now set for the simulation so you can launch the pandaLightPaint python code In terminal 6 type $cd catkin_ws cd to the folder you stored the code in $python pandaLightPaint.py The first time you run this you will get some error messages as the robot starts at singularity in the simulation. After about a minute it will find a valid motion plan to get out of singularity. At this point cancel the execution using ctrl c then start the code again. It should now run properly without errors. You should see the robot drawing individual frames in both gazebo and RViz and pausing in between each frame to allow the photo to be processed.","title":"6. launching pandaLightPaint"},{"location":"Failed-Attempts/","text":"Failed Attempts Simulation Failures The main failure which occurred in the simulation was that a motion plan could be found but the execution failed. This was a confusing problem to solve as we couldn't figure out why the specified points of the frame would be out of the dexterous workspace of the end effector. Eventually, it was discovered that two people had both added code to take into account the starting position offset for the frame drawing resulting in most of the points being out of the dexterous boundaries of the Panda robot. To mitigate this issue future code that was written with better comments so that different group members understood each other's additions. Another failure which occurred was towards the begging of the project. The first code that was written attempted to work without the use of a dedicated motion planning algorithm. We thought that it would be possible to calculate a motion plan mathematically by using DH parameters and the Pseudo inverse at 0.02s intervals of time. Although we managed to get the robot arm to move in a roughly circular motion this method was not aware of any obstacles or it's own joint angle limits so would be very difficult to get working on the real robot. Panda Failures During early testing, the Panda robot would often become stuck in singularity resulting in the entire set up having to be restarted. This was usually caused by coordinated of frames being chosen outside the dexterous workspace of the robot which was mitigated by scaling down the butterfly to be drawn. The physical attachment of the custom light stick to the end effector was very difficult and should be improved in the next iteration. The magnetic docking part had to be tapped onto one side of the gripper which provided a week bendy connection which meant the light wobbled around. This could be improved by bolting the magnetic docking connector to the gripper. Another failure which occurred which was when trying to pick up the lightstick. Because computer vision wasn't used, the lightstick had to be placed in the same location during setup each time which wasted time as it would often fail to pick up the light. Photography Failures Throughout the project, several long exposure photos were taken using different apps and manually using the camera. We discovered that it is very challenging to obtain good quality light painted images from mobile apps, an example of which can be seen below. The amount of background lighting has a huge impact on the quality of the photos taken so it is crucial to keep this to a minimum.","title":"Failed Attempts"},{"location":"Failed-Attempts/#failed-attempts","text":"","title":"Failed Attempts"},{"location":"Failed-Attempts/#simulation-failures","text":"The main failure which occurred in the simulation was that a motion plan could be found but the execution failed. This was a confusing problem to solve as we couldn't figure out why the specified points of the frame would be out of the dexterous workspace of the end effector. Eventually, it was discovered that two people had both added code to take into account the starting position offset for the frame drawing resulting in most of the points being out of the dexterous boundaries of the Panda robot. To mitigate this issue future code that was written with better comments so that different group members understood each other's additions. Another failure which occurred was towards the begging of the project. The first code that was written attempted to work without the use of a dedicated motion planning algorithm. We thought that it would be possible to calculate a motion plan mathematically by using DH parameters and the Pseudo inverse at 0.02s intervals of time. Although we managed to get the robot arm to move in a roughly circular motion this method was not aware of any obstacles or it's own joint angle limits so would be very difficult to get working on the real robot.","title":"Simulation Failures"},{"location":"Failed-Attempts/#panda-failures","text":"During early testing, the Panda robot would often become stuck in singularity resulting in the entire set up having to be restarted. This was usually caused by coordinated of frames being chosen outside the dexterous workspace of the robot which was mitigated by scaling down the butterfly to be drawn. The physical attachment of the custom light stick to the end effector was very difficult and should be improved in the next iteration. The magnetic docking part had to be tapped onto one side of the gripper which provided a week bendy connection which meant the light wobbled around. This could be improved by bolting the magnetic docking connector to the gripper. Another failure which occurred which was when trying to pick up the lightstick. Because computer vision wasn't used, the lightstick had to be placed in the same location during setup each time which wasted time as it would often fail to pick up the light.","title":"Panda Failures"},{"location":"Failed-Attempts/#photography-failures","text":"Throughout the project, several long exposure photos were taken using different apps and manually using the camera. We discovered that it is very challenging to obtain good quality light painted images from mobile apps, an example of which can be seen below. The amount of background lighting has a huge impact on the quality of the photos taken so it is crucial to keep this to a minimum.","title":"Photography Failures"},{"location":"Image/","text":"Image Capture To ensure the light painting will be visible, the surrounding lights should be turned off to increase contrast, while using long exposure photography to capture each frame. Set the camera ISO to 100 to reduce the graininess of images. To reduce light entering the camera and increasing contrast, set the aperture to f/22. Ensure the entire image is in focus by using ensuring the depth of field is at its maximum depending on your camera. This can be calculated here . Mount the camera on a tripod with an external camera remote to avoid camera shake. Set the camera on \u201ctime\u201d mode, so that one press of the shutter would open the aperture and another would close the aperture. As the duration of each frame varies, this would provide a simple error-proof method. Before taking photos, focus the camera on the origin of the end-effector using autofocus and then put on manual focus for the rest of the images. With the code provided, Franka Emika would signal when to press the shutter by wiggling before and after each frame begins.","title":"Capturing Photos"},{"location":"Image/#image-capture","text":"To ensure the light painting will be visible, the surrounding lights should be turned off to increase contrast, while using long exposure photography to capture each frame. Set the camera ISO to 100 to reduce the graininess of images. To reduce light entering the camera and increasing contrast, set the aperture to f/22. Ensure the entire image is in focus by using ensuring the depth of field is at its maximum depending on your camera. This can be calculated here . Mount the camera on a tripod with an external camera remote to avoid camera shake. Set the camera on \u201ctime\u201d mode, so that one press of the shutter would open the aperture and another would close the aperture. As the duration of each frame varies, this would provide a simple error-proof method. Before taking photos, focus the camera on the origin of the end-effector using autofocus and then put on manual focus for the rest of the images. With the code provided, Franka Emika would signal when to press the shutter by wiggling before and after each frame begins.","title":"Image Capture"},{"location":"Processing%20Photos/","text":"Processing Photos Once the images for each animation frame have been captured the animation can be created by using a video editing software such as iMovie. To create the video import every image and set it to show for a very short amount of time. Alternatively, a GIF creation application could be used however this will produce a poorer quality video.","title":"Processing Photos"},{"location":"Processing%20Photos/#processing-photos","text":"Once the images for each animation frame have been captured the animation can be created by using a video editing software such as iMovie. To create the video import every image and set it to show for a very short amount of time. Alternatively, a GIF creation application could be used however this will produce a poorer quality video.","title":"Processing Photos"},{"location":"Succesful%20Achievements/","text":"Successful Achievements Final Video Overall the group is very pleased with the outcome of the project. A video of the final butterfly animation that was produced by the robot can be seen at the link here . This took approximately 30 minutes to create. What Went Well The final output video that was created is very similar to the theoretical animation created at the start of the project fully validating the human-computer-robot-reality flow of the system. The final system is adaptable for different animations that may be created in the future. People like the animation that was created and are curious as to how it was achieved. The robot can create animations very quickly. The time taken to draw each frame of the butterfly is 10-15 seconds. The main time used is just waiting for the camera to process the image so using a better camera could save even more time. The final code is very simple to understand, as it was broken down into separate functions, and offers a good base to build upon for future developments.","title":"Succesful Achievements"},{"location":"Succesful%20Achievements/#successful-achievements","text":"","title":"Successful Achievements"},{"location":"Succesful%20Achievements/#final-video","text":"Overall the group is very pleased with the outcome of the project. A video of the final butterfly animation that was produced by the robot can be seen at the link here . This took approximately 30 minutes to create.","title":"Final Video"},{"location":"Succesful%20Achievements/#what-went-well","text":"The final output video that was created is very similar to the theoretical animation created at the start of the project fully validating the human-computer-robot-reality flow of the system. The final system is adaptable for different animations that may be created in the future. People like the animation that was created and are curious as to how it was achieved. The robot can create animations very quickly. The time taken to draw each frame of the butterfly is 10-15 seconds. The main time used is just waiting for the camera to process the image so using a better camera could save even more time. The final code is very simple to understand, as it was broken down into separate functions, and offers a good base to build upon for future developments.","title":"What Went Well"},{"location":"Troubleshooting/","text":"Troubleshooting There are only 2 issues that may be encountered during normal usage of the butterfly director code. These same errors may be found in both the simulations and when running the code on the real robot. The mitigation steps vary slightly for each of these. Troubleshooting in simulation Error in terminal reads $ motion plan found but failed during execution This error occurs when the coordinates of one of the frames being drawn result in the motion plan calculated being out of the acceptable parameters for the robot to execute. To mitigate this problem first check the coordinates in the frames file to ensure none of them is extremely large or small compared to the others. If not then reduce the value of the variable frameScale in pandaLightPaint to reduce the size of the 3D drawing. This will ensure that all coordinates are easily reachable by the robot and the error will not occur. When this error happens you may need to restart RViz as well as the Gazebo simulation. Additionally, remember that the simulated robot is initially set up at a point of singularity so it is advised to manually set joint values to set the robot up in a better starting position. Troubleshooting on the real robot Error in terminal reads $ motion plan found but failed during execution or $ no motion plan found This error occurs for the same reason as in the simulation and the same steps should be carries out to fix the issue in code. When this happens the robot will crash meaning that it needs to be reset before executing the code again. To reset the robot ctrl-c all terminal windows on the computer. Unlock the robot then move it to a slightly new position. Lock the robot again and relaunch everything in the terminal. Run the updated code file and observe for any errors.","title":"Troubleshooting"},{"location":"Troubleshooting/#troubleshooting","text":"There are only 2 issues that may be encountered during normal usage of the butterfly director code. These same errors may be found in both the simulations and when running the code on the real robot. The mitigation steps vary slightly for each of these.","title":"Troubleshooting"},{"location":"Troubleshooting/#troubleshooting-in-simulation","text":"Error in terminal reads $ motion plan found but failed during execution This error occurs when the coordinates of one of the frames being drawn result in the motion plan calculated being out of the acceptable parameters for the robot to execute. To mitigate this problem first check the coordinates in the frames file to ensure none of them is extremely large or small compared to the others. If not then reduce the value of the variable frameScale in pandaLightPaint to reduce the size of the 3D drawing. This will ensure that all coordinates are easily reachable by the robot and the error will not occur. When this error happens you may need to restart RViz as well as the Gazebo simulation. Additionally, remember that the simulated robot is initially set up at a point of singularity so it is advised to manually set joint values to set the robot up in a better starting position.","title":"Troubleshooting in simulation"},{"location":"Troubleshooting/#troubleshooting-on-the-real-robot","text":"Error in terminal reads $ motion plan found but failed during execution or $ no motion plan found This error occurs for the same reason as in the simulation and the same steps should be carries out to fix the issue in code. When this happens the robot will crash meaning that it needs to be reset before executing the code again. To reset the robot ctrl-c all terminal windows on the computer. Unlock the robot then move it to a slightly new position. Lock the robot again and relaunch everything in the terminal. Run the updated code file and observe for any errors.","title":"Troubleshooting on the real robot"},{"location":"about/","text":"Team Harvey Upton Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot, compiled images into animatioon & video editing. Tom\u00e1\u0161 K\u0148aze Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot David Prior Hope Image capturing of images with long exposure photography, researched into implementing error detection. Pauline Ng Design, manufacture and assembled controllable light painting module and docking mechanism. Luke Hillery Generated simulation of motion planning in Gazebo and Rviz software, including light painting simulation for testing, assisted with animation generaion sequence. Higor Alves De Freitas Generated and processed coordinates for each frame of animation sequence by creating grasshoper definition to produce coordinate sets from any configuration of the butterfly mesh, processed coordinate data. Special Thanks To Dr Petar Kormushev Ahmad AlAttar Digby Chappell Roni Permana Saputra Francesco Cursi Ke Wang and the Robot Intelligence Lab team for the use of the Franka Emika Robot, ROS resources and helpful guidance","title":"Team"},{"location":"about/#team","text":"Harvey Upton Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot, compiled images into animatioon & video editing. Tom\u00e1\u0161 K\u0148aze Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot David Prior Hope Image capturing of images with long exposure photography, researched into implementing error detection. Pauline Ng Design, manufacture and assembled controllable light painting module and docking mechanism. Luke Hillery Generated simulation of motion planning in Gazebo and Rviz software, including light painting simulation for testing, assisted with animation generaion sequence. Higor Alves De Freitas Generated and processed coordinates for each frame of animation sequence by creating grasshoper definition to produce coordinate sets from any configuration of the butterfly mesh, processed coordinate data.","title":"Team"},{"location":"about/#special-thanks-to","text":"Dr Petar Kormushev Ahmad AlAttar Digby Chappell Roni Permana Saputra Francesco Cursi Ke Wang and the Robot Intelligence Lab team for the use of the Franka Emika Robot, ROS resources and helpful guidance","title":"Special Thanks To"},{"location":"animationcreation/","text":"Animation Creation The butterfly model was simplified to a low-poly model since it was expected that a large number of frames would be needed to form a smooth animation. I was also intended that the focus was on the animation itself and not on the model. As the project was limited in terms of time with the robot, a simplified outline presented the most opportunity to iterate and increase robotic complexity. Ie. A low fidelity model reduces the amount of time necessary for the robot to complete each frame. Using slow-motion footage of butterflies in flight, one model for each frame of the animation was created in Rhinoceros 3D . The animation was simplified to 11 frames. The angles of the butterfly wings about the x-axis and the z-axis are approximately sinusoidal. A subtle body wiggle was also added, which is a small, random rotation of the two body triangles about the y-axis. Angles about axes are shown in this video . *NOTE Grasshopper is a plugin included in Rhino 6, which has built-in functions called 'components' to allow for easy data manipulation. Butterfly Animation To create your own butterfly animation, follow these steps below: 1. Open Rhino 6. 2. Draw an equilateral triangle on the XY plane, with one vertex on the origin. 3. With the mirror command, mirror the triangle through the XZ plane to form the butterfly wings. 4. Draw an isosceles triangle on the XZ plane with the vertex corresponding to the smallest angle to the origin. 5. Using the mirror command, mirror the triangle through the XY plane to form the butterfly body. 6. \"CTRL +G\" to group all four triangles. 7. Copy the group to get 11 instances in total where 1 is for each frame. 8. Isolate each group to it's separate layer by using the 'Change Layer Command'. 9. Rotate the wings about the X and Z axes according to with the reference of a butterfly flight footage incrementally over 11 frames. 10. Open the Grasshopper plugin. 11. Open the stl_file_to_edges_and_coordinates.gh file in Grasshopper. 12. Right-click on the first block in each frame and click 'Set Multiple Meshes'. 13. Select the group in the associated layer. Ie. Layer 1 group for frame 1. 14. Wait for the algorithm to run. 15. Right-click on the last block for each frame and select 'Copy Data Only'. 16. Paste the data in your preferred text editor in the correct frame order. 17. Replace all curly brackets with square brackets. 18. Format the data so that it looks like the list in coordinate_list_about-origin.py The stl_file_to_edges_and_coordinates.gh file can be found here The coordinate_list_about-origin.py file can be found here General Animation These are the general steps which can be repeated to output the same result for each frame in Grasshopper of your animation of choice: 1. Deconstruct a single image into vertices, faces, colours and normals using the 'deconstruct mesh' components. 2. The output will be a list of a list, where each vertex is in an XYZ coordinate. 3. Remove any repeated centre vertices by putting the lists through the 'create set' component. 4. Extract each sublist using the 'list item' component. 5. Recorded the list for the robot to move from coordinate to coordinate without retracing the same path twice. *NOTE This is important to avoid any brighter spots in the animation frames which would be seen as an anomaly. 6. The list of coordinates will be in curly brackets, replace them with square brackets. This general method allows for the flexibility of changing the butterfly animation frame by frame without the need to manually change each coordinate. This would also allow for more complexity to be added easily on the butterfly models (ie. higher poly models), at the cost of longer per-frame drawing time. The Grasshopper definition shown in the figure below is robust enough to deal with having curved lines and would output these as a higher number of interpolated points to form the curves.","title":"Creating Animation"},{"location":"animationcreation/#animation-creation","text":"The butterfly model was simplified to a low-poly model since it was expected that a large number of frames would be needed to form a smooth animation. I was also intended that the focus was on the animation itself and not on the model. As the project was limited in terms of time with the robot, a simplified outline presented the most opportunity to iterate and increase robotic complexity. Ie. A low fidelity model reduces the amount of time necessary for the robot to complete each frame. Using slow-motion footage of butterflies in flight, one model for each frame of the animation was created in Rhinoceros 3D . The animation was simplified to 11 frames. The angles of the butterfly wings about the x-axis and the z-axis are approximately sinusoidal. A subtle body wiggle was also added, which is a small, random rotation of the two body triangles about the y-axis. Angles about axes are shown in this video . *NOTE Grasshopper is a plugin included in Rhino 6, which has built-in functions called 'components' to allow for easy data manipulation.","title":"Animation Creation"},{"location":"animationcreation/#butterfly-animation","text":"To create your own butterfly animation, follow these steps below: 1. Open Rhino 6. 2. Draw an equilateral triangle on the XY plane, with one vertex on the origin. 3. With the mirror command, mirror the triangle through the XZ plane to form the butterfly wings. 4. Draw an isosceles triangle on the XZ plane with the vertex corresponding to the smallest angle to the origin. 5. Using the mirror command, mirror the triangle through the XY plane to form the butterfly body. 6. \"CTRL +G\" to group all four triangles. 7. Copy the group to get 11 instances in total where 1 is for each frame. 8. Isolate each group to it's separate layer by using the 'Change Layer Command'. 9. Rotate the wings about the X and Z axes according to with the reference of a butterfly flight footage incrementally over 11 frames. 10. Open the Grasshopper plugin. 11. Open the stl_file_to_edges_and_coordinates.gh file in Grasshopper. 12. Right-click on the first block in each frame and click 'Set Multiple Meshes'. 13. Select the group in the associated layer. Ie. Layer 1 group for frame 1. 14. Wait for the algorithm to run. 15. Right-click on the last block for each frame and select 'Copy Data Only'. 16. Paste the data in your preferred text editor in the correct frame order. 17. Replace all curly brackets with square brackets. 18. Format the data so that it looks like the list in coordinate_list_about-origin.py The stl_file_to_edges_and_coordinates.gh file can be found here The coordinate_list_about-origin.py file can be found here","title":"Butterfly Animation"},{"location":"animationcreation/#general-animation","text":"These are the general steps which can be repeated to output the same result for each frame in Grasshopper of your animation of choice: 1. Deconstruct a single image into vertices, faces, colours and normals using the 'deconstruct mesh' components. 2. The output will be a list of a list, where each vertex is in an XYZ coordinate. 3. Remove any repeated centre vertices by putting the lists through the 'create set' component. 4. Extract each sublist using the 'list item' component. 5. Recorded the list for the robot to move from coordinate to coordinate without retracing the same path twice. *NOTE This is important to avoid any brighter spots in the animation frames which would be seen as an anomaly. 6. The list of coordinates will be in curly brackets, replace them with square brackets. This general method allows for the flexibility of changing the butterfly animation frame by frame without the need to manually change each coordinate. This would also allow for more complexity to be added easily on the butterfly models (ie. higher poly models), at the cost of longer per-frame drawing time. The Grasshopper definition shown in the figure below is robust enough to deal with having curved lines and would output these as a higher number of interpolated points to form the curves.","title":"General Animation"},{"location":"betatest/","text":"Lo-Fi Beta Testing To validate the concept, a beta test was first conducted without the robotic arm. In this experiment, a human, an iPhone torch, a template, a stand, a camera and a tripod were used. The template was rotated at approximately 20\u00b0 intervals through 180\u00b0, pivoting on the stand. At each rotation, a long exposure image was taken on the camera and the human moved the light around the frame. Once this sequence was complete, the images were manually compiled in a video editing software to create the beta test animation. Steps Carried Out Mount camera on tripod & template about stand Open camera shutter Trace image with a torch Close camera shutter Rotate template approximately 20\u00b0 about stand Repeat steps 2 to 5 Import images Compile images Click here to watch the beta testing video.","title":"Lo-Fi Development"},{"location":"betatest/#lo-fi-beta-testing","text":"To validate the concept, a beta test was first conducted without the robotic arm. In this experiment, a human, an iPhone torch, a template, a stand, a camera and a tripod were used. The template was rotated at approximately 20\u00b0 intervals through 180\u00b0, pivoting on the stand. At each rotation, a long exposure image was taken on the camera and the human moved the light around the frame. Once this sequence was complete, the images were manually compiled in a video editing software to create the beta test animation.","title":"Lo-Fi Beta Testing"},{"location":"betatest/#steps-carried-out","text":"Mount camera on tripod & template about stand Open camera shutter Trace image with a torch Close camera shutter Rotate template approximately 20\u00b0 about stand Repeat steps 2 to 5 Import images Compile images Click here to watch the beta testing video.","title":"Steps Carried Out"},{"location":"error/","text":"Implementing error checking for Panda with Frankx Operating Franka Emika Panda can lead to errors due to your code, motion planning or collision. Fortunately, there are libraries which can be installed to automatically recover from errors, avoiding damage to the robot and allowing the code to rerun following an error. This tutorial uses Frankx , which is a python wrapper for libfranka (which runs in C++). The library makes real-time trajectory generation easy, allowing the robot to respond to unforeseen events. The following steps can be followed to implement error checking Install the following: libfranka Reflexxes(trajectory-generator) Eigen (transformation calculations) pybind11 (Python bindings) 2. Install frankx via: mkdir -p build cd build cmake -DReflexxes_ROOT_DIR=../RMLTypeII -DREFLEXXES_TYPE=ReflexxesTypeII -DBUILD_TYPE=Release .. make -j4 make install 3. To recover from errors include the line: robot.recover_from_errors() This allows the robot to automatically recover from errors by resetting the robot.","title":"Error Detection"},{"location":"error/#implementing-error-checking-for-panda-with-frankx","text":"Operating Franka Emika Panda can lead to errors due to your code, motion planning or collision. Fortunately, there are libraries which can be installed to automatically recover from errors, avoiding damage to the robot and allowing the code to rerun following an error. This tutorial uses Frankx , which is a python wrapper for libfranka (which runs in C++). The library makes real-time trajectory generation easy, allowing the robot to respond to unforeseen events.","title":"Implementing error checking for Panda with Frankx"},{"location":"error/#the-following-steps-can-be-followed-to-implement-error-checking","text":"Install the following: libfranka Reflexxes(trajectory-generator) Eigen (transformation calculations) pybind11 (Python bindings) 2. Install frankx via: mkdir -p build cd build cmake -DReflexxes_ROOT_DIR=../RMLTypeII -DREFLEXXES_TYPE=ReflexxesTypeII -DBUILD_TYPE=Release .. make -j4 make install 3. To recover from errors include the line: robot.recover_from_errors() This allows the robot to automatically recover from errors by resetting the robot.","title":"The following steps can be followed to implement error checking"},{"location":"evaluation/","text":"Future Development The following section includes future development aspects that can be implemented to further develop this project. Automating Image Capture This could be implemented through designing a custom shutter with a large surface area connected to the camera. The shutter could then be pressed by the robot in-between frames to open and close the aperture before and at the end of the motion. Another option could be to use an infrared sensor which the robot would need to swipe over, this would require less advanced hardware and could prevent damage to robot due to collision. The robot operating sequence would in turn change with this setup, removing the need for a wiggle indication and shifting the priority towards turning the light on and off with the gripper preventing a streak in each frame. Image Compilation This would be impossible using the current hardware available since the images are stored in an SD card on the camera. A way for the stop motion animation to be automatically compiled is by using a Raspberry Pi with a camera module. This can be implemented by appending each image to a sequence of images followed by a command to export it as a video. This would, therefore, allow the images to be automatically compiled as an animation at the end of the robot operating sequence. Speed Control The intensity of the light in an image is proportional to the time spent in each position ie. speed, s ; the brightness of the torch, bt ; and angle of motion relative to the orbiting camera angle, a which varies sinusoidally, from maximum when a=0 . Image brightness, B = (k* bt + x* sin(a))/ s Where k and x are constants. Thus, setting B to a constant and defining the pose and position of the camera would mean that the speed could be calculated to maintain a constant brightness in the light trail. However, in this project, the images taken were clear despite not implementing speed control and thus was considered a redundancy. Perception and Object Detection The mechanism of picking up the torch for this project was executed by feeding it to the robot arm at a precise location. Implementing perception will provide the robot with the ability to identify the environment by the addition of a sensor such as an RGB-D camera, a webcam or a laser scanner. This would also allow for the robot to identify the light module as a target for a pick operation","title":"Future Development"},{"location":"evaluation/#future-development","text":"The following section includes future development aspects that can be implemented to further develop this project.","title":"Future Development"},{"location":"evaluation/#automating-image-capture","text":"This could be implemented through designing a custom shutter with a large surface area connected to the camera. The shutter could then be pressed by the robot in-between frames to open and close the aperture before and at the end of the motion. Another option could be to use an infrared sensor which the robot would need to swipe over, this would require less advanced hardware and could prevent damage to robot due to collision. The robot operating sequence would in turn change with this setup, removing the need for a wiggle indication and shifting the priority towards turning the light on and off with the gripper preventing a streak in each frame.","title":"Automating Image Capture"},{"location":"evaluation/#image-compilation","text":"This would be impossible using the current hardware available since the images are stored in an SD card on the camera. A way for the stop motion animation to be automatically compiled is by using a Raspberry Pi with a camera module. This can be implemented by appending each image to a sequence of images followed by a command to export it as a video. This would, therefore, allow the images to be automatically compiled as an animation at the end of the robot operating sequence.","title":"Image Compilation"},{"location":"evaluation/#speed-control","text":"The intensity of the light in an image is proportional to the time spent in each position ie. speed, s ; the brightness of the torch, bt ; and angle of motion relative to the orbiting camera angle, a which varies sinusoidally, from maximum when a=0 . Image brightness, B = (k* bt + x* sin(a))/ s Where k and x are constants. Thus, setting B to a constant and defining the pose and position of the camera would mean that the speed could be calculated to maintain a constant brightness in the light trail. However, in this project, the images taken were clear despite not implementing speed control and thus was considered a redundancy.","title":"Speed Control"},{"location":"evaluation/#perception-and-object-detection","text":"The mechanism of picking up the torch for this project was executed by feeding it to the robot arm at a precise location. Implementing perception will provide the robot with the ability to identify the environment by the addition of a sensor such as an RGB-D camera, a webcam or a laser scanner. This would also allow for the robot to identify the light module as a target for a pick operation","title":"Perception and Object Detection"},{"location":"fail/","text":"Failed Attempts Simulation Failures The main failure which occurred in the simulation was that a motion plan could be found but the execution failed. This was a confusing problem to solve as we couldn't figure out why the specified points of the frame would be out of the dexterous workspace of the end effector. Eventually, it was discovered that two people had both added code to take into account the starting position offset for the frame drawing resulting in most of the points being out of the dexterous boundaries of the Panda robot. To mitigate this issue future code that was written with better comments so that different group members understood each other's additions. Another failure which occurred was towards the begging of the project. The first code that was written attempted to work without the use of a dedicated motion planning algorithm. We thought that it would be possible to calculate a motion plan mathematically by using DH parameters and the Pseudo inverse at 0.02s intervals of time. Although we managed to get the robot arm to move in a roughly circular motion this method was not aware of any obstacles or it's own joint angle limits so would be very difficult to get working on the real robot. Panda Failures During early testing, the Panda robot would often become stuck in singularity resulting in the entire set up having to be restarted. This was usually caused by coordinated of frames being chosen outside the dexterous workspace of the robot which was mitigated by scaling down the butterfly to be drawn. The physical attachment of the custom light stick to the end effector was very difficult and should be improved in the next iteration. The magnetic docking part had to be tapped onto one side of the gripper which provided a week bendy connection which meant the light wobbled around. This could be improved by bolting the magnetic docking connector to the gripper. Another failure which occurred which was when trying to pick up the lightstick. Because computer vision wasn't used, the lightstick had to be placed in the same location during setup each time which wasted time as it would often fail to pick up the light. Photography Failures Throughout the project, several long exposure photos were taken using different apps and manually using the camera. We discovered that it is very challenging to obtain good quality light painted images from mobile apps, an example of which can be seen below. The amount of background lighting has a huge impact on the quality of the photos taken so it is crucial to keep this to a minimum.","title":"Failed Attempts"},{"location":"fail/#failed-attempts","text":"","title":"Failed Attempts"},{"location":"fail/#simulation-failures","text":"The main failure which occurred in the simulation was that a motion plan could be found but the execution failed. This was a confusing problem to solve as we couldn't figure out why the specified points of the frame would be out of the dexterous workspace of the end effector. Eventually, it was discovered that two people had both added code to take into account the starting position offset for the frame drawing resulting in most of the points being out of the dexterous boundaries of the Panda robot. To mitigate this issue future code that was written with better comments so that different group members understood each other's additions. Another failure which occurred was towards the begging of the project. The first code that was written attempted to work without the use of a dedicated motion planning algorithm. We thought that it would be possible to calculate a motion plan mathematically by using DH parameters and the Pseudo inverse at 0.02s intervals of time. Although we managed to get the robot arm to move in a roughly circular motion this method was not aware of any obstacles or it's own joint angle limits so would be very difficult to get working on the real robot.","title":"Simulation Failures"},{"location":"fail/#panda-failures","text":"During early testing, the Panda robot would often become stuck in singularity resulting in the entire set up having to be restarted. This was usually caused by coordinated of frames being chosen outside the dexterous workspace of the robot which was mitigated by scaling down the butterfly to be drawn. The physical attachment of the custom light stick to the end effector was very difficult and should be improved in the next iteration. The magnetic docking part had to be tapped onto one side of the gripper which provided a week bendy connection which meant the light wobbled around. This could be improved by bolting the magnetic docking connector to the gripper. Another failure which occurred which was when trying to pick up the lightstick. Because computer vision wasn't used, the lightstick had to be placed in the same location during setup each time which wasted time as it would often fail to pick up the light.","title":"Panda Failures"},{"location":"fail/#photography-failures","text":"Throughout the project, several long exposure photos were taken using different apps and manually using the camera. We discovered that it is very challenging to obtain good quality light painted images from mobile apps, an example of which can be seen below. The amount of background lighting has a huge impact on the quality of the photos taken so it is crucial to keep this to a minimum.","title":"Photography Failures"},{"location":"hardware/","text":"Getting started This section details how to connect the robot and test the setup by using FCI to read the current state. Operating the robot Before going further though, here are a few safety considerations. Always check the following things before powering on the robot: Make sure that the arm has been mounted on a stable base and cannot topple over, even when performing fast motions or abrupt stops. CAUTION! Only tabletop mounting is supported, i.e. the Arm must be mounted perpendicular to the ground! Other mountings will void your warranty and might cause damage to the robot! Ensure that the cable connecting Arm and Control is firmly attached on both sides. Connect the external activation device to Arm\u2019s base and keep it next to you in order to be able to stop the robot at any time. HINT! Activating the external activation device will disconnect the Arm from Control. The joint motor controllers will then hold their current position. The external activation device is not an emergency stop! IMPORTANT! The workstation PC which commands your robot using the FCI must always be connected to the LAN port of Control (shop floor network) and not to the LAN port of the Arm (robot network). Please read the chapter dedicated to safety in the manual delivered with your robot in addition to this. Setting up the network Good network performance is crucial when controlling the robot using FCI. Therefore it is strongly recommended to use a direct connection between the workstation PC and Panda\u2019s Control. This section describes how to configure your network for this use case. NOTE! Use Control\u2019s LAN port when controlling the robot through FCI. Do not connect to the port in Arm\u2019s base. The Control and your workstation must be configured to appear on the same network. Simplest way to achieve that is to use static IP addresses. Any two addresses on the same network would work, but the following values will be used for the purpose of this tutorial: Workstation PC Control Address 172.16.0.1 172.16.0.2 Netmask 24 24 The Control\u2019s address (172.16.0.2) is called ```fci-ip```. The configuration process consists of two steps: Configuring Control\u2019s network settings. Configuring your workstation\u2019s network settings. Which can be found in detailed steps on the franka emika documentation here Once setting up, you are ready to follow through with the following sections. Franka Emika Software Software updates can be found here The software versions currently used in the robotics lab are: Software Version Franka Firmware 1.0.9 Supports libfranka < 0.2.0 ros-kinetic-libfranka 0.1.0 Current ROS version 0.2.0","title":"Hardware"},{"location":"hardware/#getting-started","text":"This section details how to connect the robot and test the setup by using FCI to read the current state.","title":"Getting started"},{"location":"hardware/#operating-the-robot","text":"Before going further though, here are a few safety considerations. Always check the following things before powering on the robot: Make sure that the arm has been mounted on a stable base and cannot topple over, even when performing fast motions or abrupt stops. CAUTION! Only tabletop mounting is supported, i.e. the Arm must be mounted perpendicular to the ground! Other mountings will void your warranty and might cause damage to the robot! Ensure that the cable connecting Arm and Control is firmly attached on both sides. Connect the external activation device to Arm\u2019s base and keep it next to you in order to be able to stop the robot at any time. HINT! Activating the external activation device will disconnect the Arm from Control. The joint motor controllers will then hold their current position. The external activation device is not an emergency stop! IMPORTANT! The workstation PC which commands your robot using the FCI must always be connected to the LAN port of Control (shop floor network) and not to the LAN port of the Arm (robot network). Please read the chapter dedicated to safety in the manual delivered with your robot in addition to this.","title":"Operating the robot"},{"location":"hardware/#setting-up-the-network","text":"Good network performance is crucial when controlling the robot using FCI. Therefore it is strongly recommended to use a direct connection between the workstation PC and Panda\u2019s Control. This section describes how to configure your network for this use case. NOTE! Use Control\u2019s LAN port when controlling the robot through FCI. Do not connect to the port in Arm\u2019s base. The Control and your workstation must be configured to appear on the same network. Simplest way to achieve that is to use static IP addresses. Any two addresses on the same network would work, but the following values will be used for the purpose of this tutorial: Workstation PC Control Address 172.16.0.1 172.16.0.2 Netmask 24 24 The Control\u2019s address (172.16.0.2) is called ```fci-ip```. The configuration process consists of two steps: Configuring Control\u2019s network settings. Configuring your workstation\u2019s network settings. Which can be found in detailed steps on the franka emika documentation here Once setting up, you are ready to follow through with the following sections.","title":"Setting up the network"},{"location":"hardware/#franka-emika-software","text":"Software updates can be found here The software versions currently used in the robotics lab are: Software Version Franka Firmware 1.0.9 Supports libfranka < 0.2.0 ros-kinetic-libfranka 0.1.0 Current ROS version 0.2.0","title":"Franka Emika Software"},{"location":"lightsource/","text":"Light Source A custom light torch was designed and manufactured to comply towards the dimensions of the Franka Emika end-effector gripper. The STL file of the light stick can be found here and be FDM printed on 0.2mm layer height at 30% infill. The light unit was designed as a hexagonal shape to be held securely on the end effector with a cone tip to direct the light source towards the centre of the unit. A second iteration was made to allow for the unit to be picked up automatically along with a single pole single throw (SPST) switch attached for the light source to stay on when the end effector gripper is closed. Attachment To add in the motion of the light stick being picked up, a magnetic strip was designed to be permanently attached on one end of the end effector. Magnets of the opposite polarity were attached on the main light unit to ensure it remains attached on the end effector when the gripper is opened. Circuitry The internal circuitry of the light unit consists of two 1.5V AA Batteries, connected to an SPST switch and an LED. This would ensure that when the end effector gripper is closed, the light is turned on and vice versa when the gripper is open. This would also remove unwanted bright spots and allow for the possibility of continuous images within the same frame. Links Links to purchase external components AA Batteries SPST Switch Magnets LED","title":"Panda Torch"},{"location":"lightsource/#light-source","text":"A custom light torch was designed and manufactured to comply towards the dimensions of the Franka Emika end-effector gripper. The STL file of the light stick can be found here and be FDM printed on 0.2mm layer height at 30% infill. The light unit was designed as a hexagonal shape to be held securely on the end effector with a cone tip to direct the light source towards the centre of the unit. A second iteration was made to allow for the unit to be picked up automatically along with a single pole single throw (SPST) switch attached for the light source to stay on when the end effector gripper is closed.","title":"Light Source"},{"location":"lightsource/#attachment","text":"To add in the motion of the light stick being picked up, a magnetic strip was designed to be permanently attached on one end of the end effector. Magnets of the opposite polarity were attached on the main light unit to ensure it remains attached on the end effector when the gripper is opened.","title":"Attachment"},{"location":"lightsource/#circuitry","text":"The internal circuitry of the light unit consists of two 1.5V AA Batteries, connected to an SPST switch and an LED. This would ensure that when the end effector gripper is closed, the light is turned on and vice versa when the gripper is open. This would also remove unwanted bright spots and allow for the possibility of continuous images within the same frame.","title":"Circuitry"},{"location":"lightsource/#links","text":"Links to purchase external components AA Batteries SPST Switch Magnets LED","title":"Links"},{"location":"methodoverview/","text":"Method Overview This project was implemented and segmented into 6 main steps 1. Animation Sequencing The model sequence of the butterfly in flight was generated using Rhinoceros3D and the Grasshopper plugin, iteratvely altering a low-poly mesh to give the effect of a butterfly flapping its wings. 2. Coordinate Generation The frame sequence was then treated as individual frames, with the coordinates of each one being extracted and processed to eliminate duplicae points. The coordinates were organised to prevent duplicate paths. 3. Motion Planning A motion plan was then generated for each frame using the coordinate list created in the previous step. Code to iterate over the list of frames and plan each motion was developed. This was simulated using RViz and Gazebo. 4. Simulation Once simulated successfully, the code was implemented using Franka Emika\u2019s Panda robotic arm. 5. Implementation An end-effector attachment containing a controllable LED was simultaneously developed and manufactured to facilitate light painting 6. Image Capture Once the code successfully ran repeatedly on the Panda robot, long-exposure photgraphs were captured of the motion. These photographs were then collated and displayed in sequence to create an animation of a butterfly in flight.","title":"Methodoverview"},{"location":"methodoverview/#method-overview","text":"This project was implemented and segmented into 6 main steps","title":"Method Overview"},{"location":"methodoverview/#1-animation-sequencing","text":"The model sequence of the butterfly in flight was generated using Rhinoceros3D and the Grasshopper plugin, iteratvely altering a low-poly mesh to give the effect of a butterfly flapping its wings.","title":"1. Animation Sequencing"},{"location":"methodoverview/#2-coordinate-generation","text":"The frame sequence was then treated as individual frames, with the coordinates of each one being extracted and processed to eliminate duplicae points. The coordinates were organised to prevent duplicate paths.","title":"2. Coordinate Generation"},{"location":"methodoverview/#3-motion-planning","text":"A motion plan was then generated for each frame using the coordinate list created in the previous step. Code to iterate over the list of frames and plan each motion was developed. This was simulated using RViz and Gazebo.","title":"3. Motion Planning"},{"location":"methodoverview/#4-simulation","text":"Once simulated successfully, the code was implemented using Franka Emika\u2019s Panda robotic arm.","title":"4. Simulation"},{"location":"methodoverview/#5-implementation","text":"An end-effector attachment containing a controllable LED was simultaneously developed and manufactured to facilitate light painting","title":"5. Implementation"},{"location":"methodoverview/#6-image-capture","text":"Once the code successfully ran repeatedly on the Panda robot, long-exposure photgraphs were captured of the motion. These photographs were then collated and displayed in sequence to create an animation of a butterfly in flight.","title":"6. Image Capture"},{"location":"real/","text":"Code Execution On Real Panda Robot This section explains how to run the python code on a real Franka Emika Panda Robot. Before attempting, please follow the setup instructions in the getting started section. Setting Up Panda Robot First, release the brakes by making sure the external activation device (EAD) is pressed down. Then, in the controller web interface, click to open brakes. A clicking sound will be heard indicating the EAD button needs to be released. After releasing the EAD button, the LED light will turn white, ready to be used. Note Remember to always have a designated person holding the safety stop to prevent damage to the robot during an error. Controlling Franka & ROS With libfranka and franka_ros installed in the real-time Linux kernel, the following steps should be carried to set up the robot. Open terminal and type cd franka_ws Then, type source /devel/setup.bash Split the terminal into 5 terminals. In the following order, type in these commands in the respective terminals. In terminal 1 type $roscore In terminal 2 type $roslaunch franka_control franka_control.launch robot_ip:=192.168.0.88 load_gripper:=true In terminal 3 type $roslaunch panda_moveit_config panda_moveit.launchcontroller:=position In terminal 4 type $roslaunch panda_moveit_config moveit_rviz.launch Executing pandaLightPaint The system is now ready to run the pandaLightPaint code In terminal 5 navigate to the folder where you sorted the python code and type $python pandaLightPaint.py The robot should begin drawing the frames in the same way as in the simulation. Please refer to the other guide sections for further information on physical set up of the robot.","title":"Execution in Reality"},{"location":"real/#code-execution-on-real-panda-robot","text":"This section explains how to run the python code on a real Franka Emika Panda Robot. Before attempting, please follow the setup instructions in the getting started section.","title":"Code Execution On Real Panda Robot"},{"location":"real/#setting-up-panda-robot","text":"First, release the brakes by making sure the external activation device (EAD) is pressed down. Then, in the controller web interface, click to open brakes. A clicking sound will be heard indicating the EAD button needs to be released. After releasing the EAD button, the LED light will turn white, ready to be used. Note Remember to always have a designated person holding the safety stop to prevent damage to the robot during an error.","title":"Setting Up Panda Robot"},{"location":"real/#controlling-franka-ros","text":"With libfranka and franka_ros installed in the real-time Linux kernel, the following steps should be carried to set up the robot. Open terminal and type cd franka_ws Then, type source /devel/setup.bash Split the terminal into 5 terminals. In the following order, type in these commands in the respective terminals. In terminal 1 type $roscore In terminal 2 type $roslaunch franka_control franka_control.launch robot_ip:=192.168.0.88 load_gripper:=true In terminal 3 type $roslaunch panda_moveit_config panda_moveit.launchcontroller:=position In terminal 4 type $roslaunch panda_moveit_config moveit_rviz.launch","title":"Controlling Franka &amp; ROS"},{"location":"real/#executing-pandalightpaint","text":"The system is now ready to run the pandaLightPaint code In terminal 5 navigate to the folder where you sorted the python code and type $python pandaLightPaint.py The robot should begin drawing the frames in the same way as in the simulation. Please refer to the other guide sections for further information on physical set up of the robot.","title":"Executing pandaLightPaint"},{"location":"resource/","text":"Helpful Resources Project Resources Documentation: https://github.com/paulinengmj/DE3-TBD Source code: https://github.com/HarveyU1/LightPaintingRobot Videos Useful Resources VM Ware: download link Franka Documentation: https://frankaemika.github.io/docs/index.html Franka Support https://support.franka.de/ Rhino Download: https://www.rhino3d.com/ STL File Download here Depth of Field Calculator: https://www.dofmaster.com/dofjs.html Beta Testing Video GitHub and Git https://guides.github.com Git basics: https://git-scm.com/book/en/v2/Getting-Started-Git-Basics The super simple beginners guide to Git: http://rogerdudler.github.io/git-guide/ Understanding the workflow of git version control: https://www.git-tower.com/learn/cheat-sheets/vcs-workflow Guidance to git commands you may need in the command line: https://www.git-tower.com/blog/git-cheat-sheet/ Markdown and ReadtheDocs https://www.mkdocs.org/ https://markdown-guide.readthedocs.io/en/latest/ Cheat sheet: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Python Structuring large Python projects Python documentation on what package, module, script are: https://docs.python.org/3/tutorial/modules.html Writing Code: Python Conventions & Documentation Documentation in Python: http://docs.python-guide.org/en/latest/writing/documentation/ The PEP8 on writing your code keeping to convention (supported by PyCharm): https://www.python.org/dev/peps/pep-0008/ The PEP257 on documenting your code: https://www.python.org/dev/peps/pep-0257/ Handling errors with exceptions and raising errors: https://docs.python.org/2/tutorial/errors.html Differences between Python 2 and Python 3: http://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html Porting code from Python 2 to Python 3: https://docs.python.org/3/howto/pyporting.html Getting Started with ROS To \u2018get started\u2019 with learning ros, you may find doing the following helps you to understand ROS better: In your home directory, ensure you have set up a complete catkin workspace . Within that workspace, create a catkin package .","title":"Resources"},{"location":"resource/#helpful-resources","text":"","title":"Helpful Resources"},{"location":"resource/#project-resources","text":"Documentation: https://github.com/paulinengmj/DE3-TBD Source code: https://github.com/HarveyU1/LightPaintingRobot Videos","title":"Project Resources"},{"location":"resource/#useful-resources","text":"VM Ware: download link Franka Documentation: https://frankaemika.github.io/docs/index.html Franka Support https://support.franka.de/ Rhino Download: https://www.rhino3d.com/ STL File Download here Depth of Field Calculator: https://www.dofmaster.com/dofjs.html Beta Testing Video","title":"Useful Resources"},{"location":"resource/#github-and-git","text":"https://guides.github.com Git basics: https://git-scm.com/book/en/v2/Getting-Started-Git-Basics The super simple beginners guide to Git: http://rogerdudler.github.io/git-guide/ Understanding the workflow of git version control: https://www.git-tower.com/learn/cheat-sheets/vcs-workflow Guidance to git commands you may need in the command line: https://www.git-tower.com/blog/git-cheat-sheet/","title":"GitHub and Git"},{"location":"resource/#markdown-and-readthedocs","text":"https://www.mkdocs.org/ https://markdown-guide.readthedocs.io/en/latest/ Cheat sheet: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet","title":"Markdown and ReadtheDocs"},{"location":"resource/#python","text":"","title":"Python"},{"location":"resource/#structuring-large-python-projects","text":"Python documentation on what package, module, script are: https://docs.python.org/3/tutorial/modules.html","title":"Structuring large Python projects"},{"location":"resource/#writing-code-python-conventions-documentation","text":"Documentation in Python: http://docs.python-guide.org/en/latest/writing/documentation/ The PEP8 on writing your code keeping to convention (supported by PyCharm): https://www.python.org/dev/peps/pep-0008/ The PEP257 on documenting your code: https://www.python.org/dev/peps/pep-0257/ Handling errors with exceptions and raising errors: https://docs.python.org/2/tutorial/errors.html Differences between Python 2 and Python 3: http://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html Porting code from Python 2 to Python 3: https://docs.python.org/3/howto/pyporting.html","title":"Writing Code: Python Conventions &amp; Documentation"},{"location":"resource/#getting-started-with-ros","text":"To \u2018get started\u2019 with learning ros, you may find doing the following helps you to understand ROS better: In your home directory, ensure you have set up a complete catkin workspace . Within that workspace, create a catkin package .","title":"Getting Started with ROS"},{"location":"simulation/","text":"Simulation Setting up RViz and Gazebo for Panda Simulation Save the python file to be simulated in the VM folder catkin_ws/src/franka_gazebo/scripts Start roscore Open a new terminal and type $ roscore Start Gazebo Open a new terminal and type $ cd catkin_ws $ source level/setup.bash $ rosin gazebo_ros gazebo Add Panda Open a new terminal and type $ cd catkin_ws $ source level/setup.bash $ roslaunch franka_gazebo panda_arm_hand.launch You should now see Panda in the Gazebo emulation pointing upwards. Start MoveIt! Open a new terminal and type $ cd catkin_ws $ source devel/setup.bash $ roslaunch panda_moveit_config demo.launch rviz_tutorial:=true RViz will start Add motion planning Make sure the Planning Scene Topic is set to /planning_scene In Planning Request, change the Planning Group to panda_arm You should see Panda in RViz in the same pose as Gazebo Start the node connecting MoveIt! to Gazebo Open a new terminal $ cd catkin_ws $ cd src/panda_publisher $ python panda_publisher.py Updating the Gazebo simulation gains Open a new terminal $ cd catkin_ws $ cd src/franka_gazebo/config $ edit default.yaml Edit the file to match this [image of the yams file] Save and close the file Controlling Panda Open a new terminal $ cd catkin_ws $ cd src/franka_gazebo/scripts $ python \u2018YOUR FILE NAME\u2019.py Motion Planning Simulation Before real-life implementation of the code on the Panda robot, the entire task was tested using RViz and Gazebo simulations. This allowed for all poses and time functions to be tested without the risk of damaging the actual robot. It also made it easier to collaborate on and iteratively improve the code as it was not necessary for each team member to individually access the physical robot. The Panda model was loaded, spawned and placed into the Gazebo environment, then controlled with the Python code written by the group members. Light Painting Simulation By tracing the position of the end effector in RViz, it allowed the path of the attached light-painting module to be recreated in the RViz viewport. This method proved to be much more efficient than camera implementation within Gazebo, which required additional plugins (such as a camera sensor plugin to publish the live image to an image topic and LinkPlot3DPlugin to achieve isolated link tracing) and produced a noisier output due to the shaking exhibited within Gazebo simulations. This effectively generated a digital model of the light painting that the Panda robot would recreate in real life. To initialize the light tracing simulation in RViz, go to: Motion Planning Planned Path Links Check Panda_Hand[_] Check Show Trail [_]","title":"Simulation"},{"location":"simulation/#simulation","text":"","title":"Simulation"},{"location":"simulation/#setting-up-rviz-and-gazebo-for-panda-simulation","text":"Save the python file to be simulated in the VM folder catkin_ws/src/franka_gazebo/scripts Start roscore Open a new terminal and type $ roscore Start Gazebo Open a new terminal and type $ cd catkin_ws $ source level/setup.bash $ rosin gazebo_ros gazebo Add Panda Open a new terminal and type $ cd catkin_ws $ source level/setup.bash $ roslaunch franka_gazebo panda_arm_hand.launch You should now see Panda in the Gazebo emulation pointing upwards. Start MoveIt! Open a new terminal and type $ cd catkin_ws $ source devel/setup.bash $ roslaunch panda_moveit_config demo.launch rviz_tutorial:=true RViz will start Add motion planning Make sure the Planning Scene Topic is set to /planning_scene In Planning Request, change the Planning Group to panda_arm You should see Panda in RViz in the same pose as Gazebo Start the node connecting MoveIt! to Gazebo Open a new terminal $ cd catkin_ws $ cd src/panda_publisher $ python panda_publisher.py Updating the Gazebo simulation gains Open a new terminal $ cd catkin_ws $ cd src/franka_gazebo/config $ edit default.yaml Edit the file to match this [image of the yams file] Save and close the file Controlling Panda Open a new terminal $ cd catkin_ws $ cd src/franka_gazebo/scripts $ python \u2018YOUR FILE NAME\u2019.py","title":"Setting up RViz and Gazebo for Panda Simulation"},{"location":"simulation/#motion-planning-simulation","text":"Before real-life implementation of the code on the Panda robot, the entire task was tested using RViz and Gazebo simulations. This allowed for all poses and time functions to be tested without the risk of damaging the actual robot. It also made it easier to collaborate on and iteratively improve the code as it was not necessary for each team member to individually access the physical robot. The Panda model was loaded, spawned and placed into the Gazebo environment, then controlled with the Python code written by the group members.","title":"Motion Planning Simulation"},{"location":"simulation/#light-painting-simulation","text":"By tracing the position of the end effector in RViz, it allowed the path of the attached light-painting module to be recreated in the RViz viewport. This method proved to be much more efficient than camera implementation within Gazebo, which required additional plugins (such as a camera sensor plugin to publish the live image to an image topic and LinkPlot3DPlugin to achieve isolated link tracing) and produced a noisier output due to the shaking exhibited within Gazebo simulations. This effectively generated a digital model of the light painting that the Panda robot would recreate in real life. To initialize the light tracing simulation in RViz, go to: Motion Planning Planned Path Links Check Panda_Hand[_] Check Show Trail [_]","title":"Light Painting Simulation"},{"location":"software/","text":"Setting Up Software The following are the instructions on setting up a virtual machine for Windows or Mac Installing VMware Workstation 15 (Windows) Go to the downloads page on the VMware website Sign up for a VMware account or log in if you already have an account Accept the End User License Agreement Download the installer file; VMware-workstation-full-15.5.1-15018442.exe Run the installer Installing VMware Fusion (Mac) Go to the downloads page on the VMware website Sign up for a VMware account or log in if you already have an account Accept the End User License Agreement Download the installer file; VMware-Fusion-11.5.1-15018442.dmg Run the installer Installing ROS Virtual Machine Download the Virtual machine archive file (.zip) from here . Extract the zip archive. It contains a single virtual machine (one .vmx file and one or more .vmdk files). Run VMware Workstation (or VMware Fusion on Mac), click \u201copen an existing VM\u201d and select the .vmx file. Start the VM by clicking on the \u201cplay\u201d button. Inside the VM, log in with the credentials: login: user password learn Once you have both the VMWare and ROS Virtual Machine installed and running, you are ready to begin running simulations.","title":"Software"},{"location":"software/#setting-up-software","text":"The following are the instructions on setting up a virtual machine for Windows or Mac","title":"Setting Up Software"},{"location":"software/#installing-vmware-workstation-15-windows","text":"Go to the downloads page on the VMware website Sign up for a VMware account or log in if you already have an account Accept the End User License Agreement Download the installer file; VMware-workstation-full-15.5.1-15018442.exe Run the installer","title":"Installing VMware Workstation 15 (Windows)"},{"location":"software/#installing-vmware-fusion-mac","text":"Go to the downloads page on the VMware website Sign up for a VMware account or log in if you already have an account Accept the End User License Agreement Download the installer file; VMware-Fusion-11.5.1-15018442.dmg Run the installer","title":"Installing VMware Fusion (Mac)"},{"location":"software/#installing-ros-virtual-machine","text":"Download the Virtual machine archive file (.zip) from here . Extract the zip archive. It contains a single virtual machine (one .vmx file and one or more .vmdk files). Run VMware Workstation (or VMware Fusion on Mac), click \u201copen an existing VM\u201d and select the .vmx file. Start the VM by clicking on the \u201cplay\u201d button. Inside the VM, log in with the credentials: login: user password learn Once you have both the VMWare and ROS Virtual Machine installed and running, you are ready to begin running simulations.","title":"Installing ROS Virtual Machine"},{"location":"trouble/","text":"","title":"Trouble"},{"location":"vid/","text":"Reference Videos A number of reference videos have been created to help you get started with the project. Overview Video - Short video explaining the overall concept of the project with the execution of a butterfly example and output animation Code Execution Tutorial - Detailed tutorial on how to run the code in the Gazebo simulator. Use this in addition to the documentation in the project guide. Real Robot Execution - Short video documenting the Panda robot executing the code in real life Simulator Execution - Short video documenting the code being executed in the simulator with virtual light painting in RViz Links Overview Video Code Execution Tutorial Real Robot Execution Simulator Execution","title":"Videos"},{"location":"vid/#reference-videos","text":"A number of reference videos have been created to help you get started with the project. Overview Video - Short video explaining the overall concept of the project with the execution of a butterfly example and output animation Code Execution Tutorial - Detailed tutorial on how to run the code in the Gazebo simulator. Use this in addition to the documentation in the project guide. Real Robot Execution - Short video documenting the Panda robot executing the code in real life Simulator Execution - Short video documenting the code being executed in the simulator with virtual light painting in RViz","title":"Reference Videos"},{"location":"vid/#links","text":"Overview Video Code Execution Tutorial Real Robot Execution Simulator Execution","title":"Links"}]}