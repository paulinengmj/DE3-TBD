{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"about/","text":"Team Harvey Upton Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot, compiled images into animatioon & video editing. Tom\u00e1\u0161 K\u0148aze Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot David Prior Hope Image capturing of images with long exposure photography, researched intor implementing error detection. Pauline Ng Design, manufacture and assemble controllable light painting module and docking mechanism. Luke Hillery Generate simulation of motion planning in Gazebo and Rviz software, including light painting simulation for testing, assisted with animation generaion sequence. Higor Alves De Freitas Generated and processed coordinates for each frame of animation sequence by creating grasshoper definition to produce coordinate sets from any configuration of the butterfly mesh, processed coordinate data.","title":"About"},{"location":"about/#team","text":"Harvey Upton Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot, compiled images into animatioon & video editing. Tom\u00e1\u0161 K\u0148aze Co-wrote code to generate motion plan based on supplied coordinate sets, implemented code on Panda robot David Prior Hope Image capturing of images with long exposure photography, researched intor implementing error detection. Pauline Ng Design, manufacture and assemble controllable light painting module and docking mechanism. Luke Hillery Generate simulation of motion planning in Gazebo and Rviz software, including light painting simulation for testing, assisted with animation generaion sequence. Higor Alves De Freitas Generated and processed coordinates for each frame of animation sequence by creating grasshoper definition to produce coordinate sets from any configuration of the butterfly mesh, processed coordinate data.","title":"Team"},{"location":"chess/","text":"Franka Arm Starting up The Arm should be connected to the workshop floor controller with the thick black cable and the controller box should be powered up. When booting, the arm LEDs will flash yellow. Once the arm is booted (solid yellow when completed) you can release the brakes via the web interface. .. figure:: _static/franka_wiring_guide_general.png :align: center :figclass: align-center Wiring configuration for using the Franka. Log into the controller web interface (http://192.168.0.88) with: Username: robin Password: panda .. note:: Your browser may say the connection is not secure, or some other warning. If this is the case, click on the 'advanced' button, and then click 'proceed anyway'. To release the brakes on the web interface: . Make sure the external activation device (EAD) is pressed down. . On the web interface, bottom right, open the brakes. . There will be a clicking noise from the arm. . Release the EAD button and the LED light should be solid white. This means the Arm is ready to be used. .. attention:: The white light means the robot is in movement mode. The external activation device (EAD) / button is a software stop. It 'detaches' the arm from the controller which causes the arm to lock (yellow light on). It is not an emergency stop, as it does not cut power to the arm/controller. With the Arm in movement mode, it can be manually manipulated by squeezing the buttons at the end effector. The buttons have a two stage press, and if you press too hard on the buttons, the arm will lock again. The Arm will automatically go into gravity compensation mode when manually moving it. .. tip:: If the end-effector has recently been removed or readded, the gravity compensation may not be performing well. This is because the web interface settings have not been updated to account for the decrease/increase to expected weight. See the section on Removing or adding the Franka's end-effector (hand) _. .. note:: When designing motion controllers, we are recommended to use impedance control, not high gain control. This will mean we can reduce the stiffness of the arm when moving making it safer for collaborative environments. Networking information If you now want to use a workstation computer to control the Arm via the FRANKA Control Interface (FCI) libraries, first ensure you have completed the above steps to unlock the Arm brakes . Also check the ethernet cable is attached either to a network switch or directly to the shop floor controller. .. attention:: According to FRANKA documentation <https://frankaemika.github.io/docs/getting_started.html#operating-the-robot> _: \"the workstation PC which commands your robot using the FCI must always be connected to the LAN port of Control (shop floor network) and not to the LAN port of the Arm (robot network).\" With the main workstation computer should have a static IPv4 address for the computer in the Ubuntu network settings. The recommended values are seen below: ======================= ============ ============================== Device IP Address Notes ======================= ============ ============================== FRANKA Arm 192.168.1.0 This does not change Shop floor (controller) 192.168.0.88 This does not change Workstation (main) 192.168.0.77 Should be static (in settings) ======================= ============ ============================== .. important:: It is important to note that the IP address of the FRANKA Arm and shop floor controller are static and should not be changed . Use this table as reference. You can confirm that the workstation computer is able to communicate with the workshop controller by pinging the IP address from the terminal:: $ ping 192.168.0.88 .. note:: Communicating with the Franka over the switch with a static IP does allow you to have internet access, just note that the gateway and DNS settings should be provided in the Ubuntu settings accordingly to make it work. Removing or adding the Franka's end-effector (hand) To remove or add the hand, first shutdown the arm completely. Secure/remove the hand using both screws and the attach/detach the interface cable. Once the robot has restarted, go to the Settings in the web interface, go to End effector and set both the hand drop-down menu and toggle the gripper. .. _franka-emika-software: Franka Emika Software Software updates can be found at: http://support.franka.de/ The software versions currently used in the robotics lab are: ======================= ============ ============================== Software Version Notes ======================= ============ ============================== Franka Firmware 1.0.9 Supports libfranka < 0.2.0 ros-kinetic-libfranka 0.1.0 Current ROS version is 0.2.0 franka_ros ?? Currently unused ======================= ============ ============================== .. warning:: The lab only supports libfranka 0.1.0 which is currently unavailable from apt install . Do NOT uninstall ROS or libfranka on workstations which already have it installed . Shutting down the Arm Enter the web interface for the Arm. In the lower right menu, lock the brakes. Then in the top right menu, select shutdown, and confirm. .. important:: Remember to shutdown the controller from the web interface. This device is a computer, and should not be switched off from mains. Appendix In rare cases, you may need to access the Franka arm directly by connecting the ethernet cable as seen in the image below: .. figure:: _static/franka_wiring_guide_robot_arm.png :align: center :figclass: align-center Wiring configuration fo accessing the Arm directly (through the web interface). Log into the controller web interface (http://robot.franka.de) with: Username: robin Password: panda","title":"Chess"},{"location":"chess/#starting-up","text":"The Arm should be connected to the workshop floor controller with the thick black cable and the controller box should be powered up. When booting, the arm LEDs will flash yellow. Once the arm is booted (solid yellow when completed) you can release the brakes via the web interface. .. figure:: _static/franka_wiring_guide_general.png :align: center :figclass: align-center Wiring configuration for using the Franka. Log into the controller web interface (http://192.168.0.88) with: Username: robin Password: panda .. note:: Your browser may say the connection is not secure, or some other warning. If this is the case, click on the 'advanced' button, and then click 'proceed anyway'. To release the brakes on the web interface:","title":"Starting up"},{"location":"chess/#make-sure-the-external-activation-device-ead-is-pressed-down","text":"","title":". Make sure the external activation device (EAD) is pressed down."},{"location":"chess/#on-the-web-interface-bottom-right-open-the-brakes","text":"","title":". On the web interface, bottom right, open the brakes."},{"location":"chess/#there-will-be-a-clicking-noise-from-the-arm","text":"","title":". There will be a clicking noise from the arm."},{"location":"chess/#release-the-ead-button-and-the-led-light-should-be-solid-white-this-means-the-arm-is-ready-to-be-used","text":".. attention:: The white light means the robot is in movement mode. The external activation device (EAD) / button is a software stop. It 'detaches' the arm from the controller which causes the arm to lock (yellow light on). It is not an emergency stop, as it does not cut power to the arm/controller. With the Arm in movement mode, it can be manually manipulated by squeezing the buttons at the end effector. The buttons have a two stage press, and if you press too hard on the buttons, the arm will lock again. The Arm will automatically go into gravity compensation mode when manually moving it. .. tip:: If the end-effector has recently been removed or readded, the gravity compensation may not be performing well. This is because the web interface settings have not been updated to account for the decrease/increase to expected weight. See the section on Removing or adding the Franka's end-effector (hand) _. .. note:: When designing motion controllers, we are recommended to use impedance control, not high gain control. This will mean we can reduce the stiffness of the arm when moving making it safer for collaborative environments.","title":". Release the EAD button and the LED light should be solid white. This means the Arm is ready to be used."},{"location":"chess/#networking-information","text":"If you now want to use a workstation computer to control the Arm via the FRANKA Control Interface (FCI) libraries, first ensure you have completed the above steps to unlock the Arm brakes . Also check the ethernet cable is attached either to a network switch or directly to the shop floor controller. .. attention:: According to FRANKA documentation <https://frankaemika.github.io/docs/getting_started.html#operating-the-robot> _: \"the workstation PC which commands your robot using the FCI must always be connected to the LAN port of Control (shop floor network) and not to the LAN port of the Arm (robot network).\" With the main workstation computer should have a static IPv4 address for the computer in the Ubuntu network settings. The recommended values are seen below: ======================= ============ ============================== Device IP Address Notes ======================= ============ ============================== FRANKA Arm 192.168.1.0 This does not change Shop floor (controller) 192.168.0.88 This does not change Workstation (main) 192.168.0.77 Should be static (in settings) ======================= ============ ============================== .. important:: It is important to note that the IP address of the FRANKA Arm and shop floor controller are static and should not be changed . Use this table as reference. You can confirm that the workstation computer is able to communicate with the workshop controller by pinging the IP address from the terminal:: $ ping 192.168.0.88 .. note:: Communicating with the Franka over the switch with a static IP does allow you to have internet access, just note that the gateway and DNS settings should be provided in the Ubuntu settings accordingly to make it work.","title":"Networking information"},{"location":"chess/#removing-or-adding-the-frankas-end-effector-hand","text":"To remove or add the hand, first shutdown the arm completely. Secure/remove the hand using both screws and the attach/detach the interface cable. Once the robot has restarted, go to the Settings in the web interface, go to End effector and set both the hand drop-down menu and toggle the gripper. .. _franka-emika-software:","title":"Removing or adding the Franka's end-effector (hand)"},{"location":"chess/#franka-emika-software","text":"Software updates can be found at: http://support.franka.de/ The software versions currently used in the robotics lab are: ======================= ============ ============================== Software Version Notes ======================= ============ ============================== Franka Firmware 1.0.9 Supports libfranka < 0.2.0 ros-kinetic-libfranka 0.1.0 Current ROS version is 0.2.0 franka_ros ?? Currently unused ======================= ============ ============================== .. warning:: The lab only supports libfranka 0.1.0 which is currently unavailable from apt install . Do NOT uninstall ROS or libfranka on workstations which already have it installed .","title":"Franka Emika Software"},{"location":"chess/#shutting-down-the-arm","text":"Enter the web interface for the Arm. In the lower right menu, lock the brakes. Then in the top right menu, select shutdown, and confirm. .. important:: Remember to shutdown the controller from the web interface. This device is a computer, and should not be switched off from mains.","title":"Shutting down the Arm"},{"location":"chess/#appendix","text":"In rare cases, you may need to access the Franka arm directly by connecting the ethernet cable as seen in the image below: .. figure:: _static/franka_wiring_guide_robot_arm.png :align: center :figclass: align-center Wiring configuration fo accessing the Arm directly (through the web interface). Log into the controller web interface (http://robot.franka.de) with: Username: robin Password: panda","title":"Appendix"},{"location":"evaluation/","text":"Outcomes Error Detection The chances of error were significantly mitigated by ensuring the motion plan to only be within the workspace of the robot, therefore reducing chances of motion planning error due to collision or being out of the task space. However, error detection and recovery could have been built into the code by using Frankx, which is a high-level motion planning library for Franka Emika Panda. Frankx is a python wrapper for Libfranka (which only works in C++), using Reflexes as a trajectory generator, Eigen for transformation calculations and pybind11 for Python bindings. Once installed, the command robot.recover_from_errors() allows the robot to automatically recover from an error by resetting the robot. Missed Opportunities The following section includes missed opportunities that could have been implemented given a longer project time period. Automating Image Capture This could have been implemented through designing a custom shutter with a large surface area connected to the camera. The shutter could then be pressed by the robot in-between frames to open and close the aperture before and at the end of the motion. Another option could be to use an infrared sensor which the robot would need to swipe over, this would require less advanced hardware and could prevent damage of the robot due to collision. The robot operating sequence would in turn change with this setup, without the need for a wiggle indication and to shift the priority towards turning the light on and off with the gripper becoming to prevent a streak in each frame. Image Compilation This was challenging using the hardware available since the images were stored in an SD card. A way for the stop motion animation to be automatically compiled is by using a Raspberry Pi with a camera module. In the code, each image could be appended to a sequence of images and then exported as a video. This would then be able to automatically compile an animation at the end of the robot operating sequence. Speed Control Speed control could have been implemented. The intensity of light in an image was proportional to the time spent in each position (ie. speed, s); brightness of the torch, bt; and angle of motion relative to the orbiting camera angle, a (varying sinusoidally, from maximum when a = 90 degrees, to minimum when a = 0).Image brightness, B = (k bt + x sin(a)) / s Where k and x are constants. Given the above equation, setting B to a constant and defining the pose and position of the camera would mean that the speed could be calculated to maintain a constant brightness in the light trail. Ultimately, however, the images proved to be aesthetic without this, thus it was considered a redundancy. Picking up the torch was executed by directing the robot arm to the correct position and then allowing the torch to snap on using magnets. This assumes that the resting position and pose of the torch is always the same. An opportunity could have been to include computer vision to locate the current position and pose of the torch and then generate a motion plan to pick up the torch. This would have been a more robust method, however, would drastically increase the complexity of the robot by including image processing. Potentially the image processing could be done using a Raspberry Pi camera (same as the one for frame photography) which could provide data to create a motion plan to pick up the torch.","title":"Evaluation"},{"location":"evaluation/#outcomes","text":"","title":"Outcomes"},{"location":"evaluation/#error-detection","text":"The chances of error were significantly mitigated by ensuring the motion plan to only be within the workspace of the robot, therefore reducing chances of motion planning error due to collision or being out of the task space. However, error detection and recovery could have been built into the code by using Frankx, which is a high-level motion planning library for Franka Emika Panda. Frankx is a python wrapper for Libfranka (which only works in C++), using Reflexes as a trajectory generator, Eigen for transformation calculations and pybind11 for Python bindings. Once installed, the command robot.recover_from_errors() allows the robot to automatically recover from an error by resetting the robot.","title":"Error Detection"},{"location":"evaluation/#missed-opportunities","text":"The following section includes missed opportunities that could have been implemented given a longer project time period. Automating Image Capture This could have been implemented through designing a custom shutter with a large surface area connected to the camera. The shutter could then be pressed by the robot in-between frames to open and close the aperture before and at the end of the motion. Another option could be to use an infrared sensor which the robot would need to swipe over, this would require less advanced hardware and could prevent damage of the robot due to collision. The robot operating sequence would in turn change with this setup, without the need for a wiggle indication and to shift the priority towards turning the light on and off with the gripper becoming to prevent a streak in each frame. Image Compilation This was challenging using the hardware available since the images were stored in an SD card. A way for the stop motion animation to be automatically compiled is by using a Raspberry Pi with a camera module. In the code, each image could be appended to a sequence of images and then exported as a video. This would then be able to automatically compile an animation at the end of the robot operating sequence. Speed Control Speed control could have been implemented. The intensity of light in an image was proportional to the time spent in each position (ie. speed, s); brightness of the torch, bt; and angle of motion relative to the orbiting camera angle, a (varying sinusoidally, from maximum when a = 90 degrees, to minimum when a = 0).Image brightness, B = (k bt + x sin(a)) / s Where k and x are constants. Given the above equation, setting B to a constant and defining the pose and position of the camera would mean that the speed could be calculated to maintain a constant brightness in the light trail. Ultimately, however, the images proved to be aesthetic without this, thus it was considered a redundancy. Picking up the torch was executed by directing the robot arm to the correct position and then allowing the torch to snap on using magnets. This assumes that the resting position and pose of the torch is always the same. An opportunity could have been to include computer vision to locate the current position and pose of the torch and then generate a motion plan to pick up the torch. This would have been a more robust method, however, would drastically increase the complexity of the robot by including image processing. Potentially the image processing could be done using a Raspberry Pi camera (same as the one for frame photography) which could provide data to create a motion plan to pick up the torch.","title":"Missed Opportunities"},{"location":"intro/","text":"Introduction Drawing inspiration from light painting images, this project entails programming the Franka Emika robotic arm to draw several still images of a butterfly with light to produce a stop motion animation. Lo-Fi Beta Testing To validate the concept, a beta test was first conducted without the robotic arm. In this experiment, a human, an iPhone torch, a template, a stand, a camera and a tripod were used. The template was rotated at approximately 20 degree intervals through 180 degrees, pivoting on the stand. At each rotation, a long exposure image was taken on the camera and the human moved the light around the frame. Once this sequence was complete, the images were manually compiled in a video editing software to create the beta test animation.","title":"Getting Started"},{"location":"intro/#introduction","text":"Drawing inspiration from light painting images, this project entails programming the Franka Emika robotic arm to draw several still images of a butterfly with light to produce a stop motion animation.","title":"Introduction"},{"location":"intro/#lo-fi-beta-testing","text":"To validate the concept, a beta test was first conducted without the robotic arm. In this experiment, a human, an iPhone torch, a template, a stand, a camera and a tripod were used. The template was rotated at approximately 20 degree intervals through 180 degrees, pivoting on the stand. At each rotation, a long exposure image was taken on the camera and the human moved the light around the frame. Once this sequence was complete, the images were manually compiled in a video editing software to create the beta test animation.","title":"Lo-Fi Beta Testing"},{"location":"projectdevelopment/","text":"Project Development HELLO ! This section of the documentation consists of the development from animation creation to image capturing Method Overview This project was implemented and segmented into 6 main steps 1. Animation Sequencing The model sequence of the butterfly in flight was generated using Rhinoceros3D and the Grasshopper plugin, iteratvely altering a low-poly mesh to give the effect of a butterfly flapping its wings. 2. Coordinate Generation The frame sequence was then treated as individual frames, with the coordinates of each one being extracted and processed to eliminate duplicae points. The coordinates were organised to prevent duplicate paths. 3. Motion Planning A motion plan was then generated for each frame using the coordinate list created in the previous step. Code to iterate over the list of frames and plan each motion was developed. This was simulated using RViz and Gazebo. 4. Simulation Once simulated successfully, the code was implemented using Franka Emika\u2019s Panda robotic arm. 5. Implementation An end-effector attachment containing a controllable LED was simultaneously developed and manufactured to facilitate light painting 6. Image Capture Once the code successfully ran repeatedly on the Panda robot, long-exposure photgraphs were captured of the motion. These photographs were then collated and displayed in sequence to create an animation of a butterfly in flight. Animation Creation The butterfly model was simplified to a low-poly model since it was expected that a large number of frames would be needed to form a smooth animation. I was also intended that the focus was on the animation itself and not on the model. As the project was limited in terms of time with the robot, a simplified outline presented the most opportunity to iterate and increase robotic complexity. I.e. A low fidelity model reduces the amount of time necessary for the robot to complete each frame. Using slow-motion footage of butterflies in flight, one model for each frame of the animation was created in Rhinoceros 3D . The animation was simplified to 11 frames. The angles of the butterfly wings about the x-axis and the z-axis are approximately sinusoidal. A subtle body wiggle was also added, which is a small, random rotation of the two body triangles about the y-axis. (Angles in reference to Figure B axes). NOTE: Grasshopper is a plugin included in Rhino 6, which has built-in functions called \u201ccomponents\u201d to allow for easy data manipulation. These are the steps which can be repeated to output the same result for each frame in Grasshopper: Each of the 4 triangles which form a butterfly were deconstructed into their vertices, faces, colours and normals using the \u201cdeconstruct mesh\u201d components. This outputs as a list of lists; a list of 4 triangles and each triangle having 3 vertices, where each vertex is an xyz coordinate. The repeated centre vertices were removed by putting the lists through the \u201ccreate set\u201d component. Each sublist was extracted using a \u201clist item\u201d components. Reorder the list in such a way that the robot can move from coordinate to coordinate without retracing the same path twice. NOTE: This is important in order to avoid any brighter spots in the animation frames which would be seen as an anomaly. The result is a list of coordinates in curly brackets. Simple data manipulation was then carried out in a text editor to put this in the required python form which uses square brackets (replace function in Wordpad, replacing \u201c{\u201c and \u201c}\u201d for \u201c[\u201c and \u201c]\u201d respectively. This method allows for the flexibility of changing the butterfly animation frame by frame without the need to manually change each coordinate. This would also allow for more complexity to be added easily on the butterfly models (ie. higher poly models), at the cost of a longer per-frame drawing time. The Grasshopper definition shown in Figure C is robust enough to deal with having curved lines, and would output these as a higher number of interpolated points to form the curves. Given more time with the robot, this change could be implemented. Motion Planning Harvey Simulation Luke (Rviz) Implementation on Panda Setting Up Franka Emika First, release the brakes by making sure the external activation device (EAD) is pressed down. Then, in the controller web interface, click to open brakes. A clicking sound will be heard indicating the EAD button needs to be released. After releasing the EAD button, the LED light will turn white, ready to be used. Controlling Franka & ROS With libfranka and franka_ros installed in the real-time Linux kernel, the following steps were carried out before launching the pre-programmed code. Open terminal and type cd franka_ws Then, type source /devel/setup.bash Split the terminal into 5 terminals. In the following order, type in these commands in the respective terminals. \u2022 Terminal 1: roscore \u2022 Terminal 2: roslaunch franka_control franka_control.launch robot_ip:=192.168.0.88 load_gripper:=true \u2022 Terminal 3: roslaunch panda_moveit_config panda_moveit. launchcontroller:=position \u2022 Terminal 4: roslaunch panda_moveit_config moveit_rviz.launch \u2022 Terminal 5: python \u2018NAMEOFCODE\u2019.py NOTE: to change the \u2018NAMEOFCODE\u2019 as the name of python file to be executed. grass Image Capture","title":"Project Development"},{"location":"projectdevelopment/#project-development","text":"HELLO ! This section of the documentation consists of the development from animation creation to image capturing","title":"Project Development"},{"location":"projectdevelopment/#method-overview","text":"This project was implemented and segmented into 6 main steps","title":"Method Overview"},{"location":"projectdevelopment/#1-animation-sequencing","text":"The model sequence of the butterfly in flight was generated using Rhinoceros3D and the Grasshopper plugin, iteratvely altering a low-poly mesh to give the effect of a butterfly flapping its wings.","title":"1. Animation Sequencing"},{"location":"projectdevelopment/#2-coordinate-generation","text":"The frame sequence was then treated as individual frames, with the coordinates of each one being extracted and processed to eliminate duplicae points. The coordinates were organised to prevent duplicate paths.","title":"2. Coordinate Generation"},{"location":"projectdevelopment/#3-motion-planning","text":"A motion plan was then generated for each frame using the coordinate list created in the previous step. Code to iterate over the list of frames and plan each motion was developed. This was simulated using RViz and Gazebo.","title":"3. Motion Planning"},{"location":"projectdevelopment/#4-simulation","text":"Once simulated successfully, the code was implemented using Franka Emika\u2019s Panda robotic arm.","title":"4. Simulation"},{"location":"projectdevelopment/#5-implementation","text":"An end-effector attachment containing a controllable LED was simultaneously developed and manufactured to facilitate light painting","title":"5. Implementation"},{"location":"projectdevelopment/#6-image-capture","text":"Once the code successfully ran repeatedly on the Panda robot, long-exposure photgraphs were captured of the motion. These photographs were then collated and displayed in sequence to create an animation of a butterfly in flight.","title":"6. Image Capture"},{"location":"projectdevelopment/#animation-creation","text":"The butterfly model was simplified to a low-poly model since it was expected that a large number of frames would be needed to form a smooth animation. I was also intended that the focus was on the animation itself and not on the model. As the project was limited in terms of time with the robot, a simplified outline presented the most opportunity to iterate and increase robotic complexity. I.e. A low fidelity model reduces the amount of time necessary for the robot to complete each frame. Using slow-motion footage of butterflies in flight, one model for each frame of the animation was created in Rhinoceros 3D . The animation was simplified to 11 frames. The angles of the butterfly wings about the x-axis and the z-axis are approximately sinusoidal. A subtle body wiggle was also added, which is a small, random rotation of the two body triangles about the y-axis. (Angles in reference to Figure B axes). NOTE: Grasshopper is a plugin included in Rhino 6, which has built-in functions called \u201ccomponents\u201d to allow for easy data manipulation. These are the steps which can be repeated to output the same result for each frame in Grasshopper: Each of the 4 triangles which form a butterfly were deconstructed into their vertices, faces, colours and normals using the \u201cdeconstruct mesh\u201d components. This outputs as a list of lists; a list of 4 triangles and each triangle having 3 vertices, where each vertex is an xyz coordinate. The repeated centre vertices were removed by putting the lists through the \u201ccreate set\u201d component. Each sublist was extracted using a \u201clist item\u201d components. Reorder the list in such a way that the robot can move from coordinate to coordinate without retracing the same path twice. NOTE: This is important in order to avoid any brighter spots in the animation frames which would be seen as an anomaly. The result is a list of coordinates in curly brackets. Simple data manipulation was then carried out in a text editor to put this in the required python form which uses square brackets (replace function in Wordpad, replacing \u201c{\u201c and \u201c}\u201d for \u201c[\u201c and \u201c]\u201d respectively. This method allows for the flexibility of changing the butterfly animation frame by frame without the need to manually change each coordinate. This would also allow for more complexity to be added easily on the butterfly models (ie. higher poly models), at the cost of a longer per-frame drawing time. The Grasshopper definition shown in Figure C is robust enough to deal with having curved lines, and would output these as a higher number of interpolated points to form the curves. Given more time with the robot, this change could be implemented.","title":"Animation Creation"},{"location":"projectdevelopment/#motion-planning","text":"Harvey","title":"Motion Planning"},{"location":"projectdevelopment/#simulation","text":"Luke (Rviz)","title":"Simulation"},{"location":"projectdevelopment/#implementation-on-panda","text":"","title":"Implementation on Panda"},{"location":"projectdevelopment/#setting-up-franka-emika","text":"First, release the brakes by making sure the external activation device (EAD) is pressed down. Then, in the controller web interface, click to open brakes. A clicking sound will be heard indicating the EAD button needs to be released. After releasing the EAD button, the LED light will turn white, ready to be used.","title":"Setting Up Franka Emika"},{"location":"projectdevelopment/#controlling-franka-ros","text":"With libfranka and franka_ros installed in the real-time Linux kernel, the following steps were carried out before launching the pre-programmed code. Open terminal and type cd franka_ws Then, type source /devel/setup.bash Split the terminal into 5 terminals. In the following order, type in these commands in the respective terminals. \u2022 Terminal 1: roscore \u2022 Terminal 2: roslaunch franka_control franka_control.launch robot_ip:=192.168.0.88 load_gripper:=true \u2022 Terminal 3: roslaunch panda_moveit_config panda_moveit. launchcontroller:=position \u2022 Terminal 4: roslaunch panda_moveit_config moveit_rviz.launch \u2022 Terminal 5: python \u2018NAMEOFCODE\u2019.py NOTE: to change the \u2018NAMEOFCODE\u2019 as the name of python file to be executed.","title":"Controlling Franka &amp; ROS"},{"location":"projectdevelopment/#grass","text":"","title":"grass"},{"location":"projectdevelopment/#image-capture","text":"","title":"Image Capture"},{"location":"projectproposal/","text":"Introduction Drawing inspiration from light painting images, this project entails programming the Franka Emika robotic arm to draw several still images of a butterfly with light to produce a stop motion animation. Lo-Fi Beta Testing To validate the concept, a beta test was first conducted without the robotic arm. In this experiment, a human, an iPhone torch, a template, a stand, a camera and a tripod were used. The template was rotated at approximately 20 degree intervals through 180 degrees, pivoting on the stand. At each rotation, a long exposure image was taken on the camera and the human moved the light around the frame. Once this sequence was complete, the images were manually compiled in a video editing software to create the beta test animation. Steps Mount camera on tripod & template about stand Open camera shutter Trace image with torch Close camera shutter Rotate template approximately 20\u00b0 about stand Repeat steps 2 to 5 Import images Compile images","title":"Project Proposal"},{"location":"projectproposal/#introduction","text":"Drawing inspiration from light painting images, this project entails programming the Franka Emika robotic arm to draw several still images of a butterfly with light to produce a stop motion animation.","title":"Introduction"},{"location":"projectproposal/#lo-fi-beta-testing","text":"To validate the concept, a beta test was first conducted without the robotic arm. In this experiment, a human, an iPhone torch, a template, a stand, a camera and a tripod were used. The template was rotated at approximately 20 degree intervals through 180 degrees, pivoting on the stand. At each rotation, a long exposure image was taken on the camera and the human moved the light around the frame. Once this sequence was complete, the images were manually compiled in a video editing software to create the beta test animation.","title":"Lo-Fi Beta Testing"},{"location":"projectproposal/#steps","text":"Mount camera on tripod & template about stand Open camera shutter Trace image with torch Close camera shutter Rotate template approximately 20\u00b0 about stand Repeat steps 2 to 5 Import images Compile images","title":"Steps"}]}